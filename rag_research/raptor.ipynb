{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing and Thematic Organization for Retrieval\n",
    "\n",
    "Here are good resources to understand the RAPTOR:\n",
    "- [rag_techniques raptor example](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb)\n",
    "- [RAPTOR official source code](https://github.com/parthsarthi03/raptor)\n",
    "- My Medium (coming soon...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: PyMuPDF in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (1.24.9)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from PyMuPDF) (1.24.9)\n",
      "Requirement already satisfied: sgmllib3k in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (2.8.2)\n",
      "Requirement already satisfied: openai in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (1.42.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: tiktoken in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: umap-learn in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (0.5.6)\n",
      "Requirement already satisfied: tenacity in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (8.5.0)\n",
      "Collecting tenacity\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: numpy in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: certifi in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: sympy in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (73.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-legacy 0.9.48.post3 requires tenacity<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "llama-index-core 0.11.1 requires tenacity!=8.4.0,<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tenacity-9.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (0.11.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.11.1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.2.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.42.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.0.15)\n",
      "Requirement already satisfied: pandas in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.2.0->llama-index) (0.5.0)\n",
      "Requirement already satisfied: click in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.7.24)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.8)\n",
      "Requirement already satisfied: sniffio in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.1->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rkyer_chang/dev-personal/rag-research/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed tenacity-8.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install nessary dependecy for the RAPTOR example\n",
    "!pip install -U arxiv PyMuPDF # for download the document for example demonstrate\n",
    "!pip install -U pydantic openai sentence-transformers transformers tiktoken scikit-learn faiss-cpu umap-learn tenacity\n",
    "!pip install -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import tempfile\n",
    "\n",
    "import typing as t\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "\n",
    "import arxiv\n",
    "import fitz\n",
    "import tiktoken\n",
    "import umap\n",
    "import tenacity\n",
    "\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # replace by your openai api key\n",
    "\n",
    "# Config\n",
    "CHUNK_SIZE = 100  # chunk size for the document\n",
    "CHUNK_OVERLAP = 0  # overlap between two chunks\n",
    "REDUCTION_COMPONENTS = 15  # number of components for UMAP\n",
    "RANDOM_STATE = 123  # random state for GMM\n",
    "MAX_LEVEL = 10  # maximum level for the clustering\n",
    "MAX_TOKEN = 5000  # maximum tokens in a local cluster\n",
    "MAX_CLUSTER_SIZE = 50  # maximum number of clusters\n",
    "THRESHOLD = 0.1  # threshold for the clustering (depends on your document)\n",
    "\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Document\n",
    "\n",
    "First, let's install required libraries and download the document for the demonstration.\n",
    " \n",
    "For the document, here choose the RAPTOR paper itself to demonstrate the RAPTOR model.\n",
    "\n",
    "Here is the paper url:\n",
    "- [RAPTOR: Recursive Abstractive Processing and Thematic Organization for Retrieval](https://arxiv.org/abs/2401.18059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArXivPaper(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the paper\")\n",
    "    text: str = Field(..., description=\"Text content of the paper\")\n",
    "    url: str = Field(..., description=\"URL of the paper on ArXiv\")\n",
    "\n",
    "\n",
    "def fetch_single_paper(url: str) -> ArXivPaper:\n",
    "    \"\"\"Fetches and downloads an ArXiv paper from the given URL and returns its ArXivPaper object.\"\"\"\n",
    "    # Ensure SSL certificate is verified\n",
    "    try:\n",
    "        urllib.request.urlopen(url)\n",
    "    except (ssl.SSLCertVerificationError, urllib.error.URLError):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    # Extract ArXiv ID from the URL\n",
    "    arxiv_match = re.search(r\"(\\d{4}\\.\\d{4,5})(v\\d+)?\", url)\n",
    "    if not arxiv_match:\n",
    "        raise ValueError(\n",
    "            f\"Invalid ArXiv URL: {url}. Expected URL should contain ArXiv ID.\"\n",
    "        )\n",
    "    arxiv_id = arxiv_match.group(1)\n",
    "\n",
    "    # Fetch paper using ArXiv ID\n",
    "    client = arxiv.Client()\n",
    "    search_query = arxiv.Search(id_list=[arxiv_id])\n",
    "    search_result = client.results(search_query)\n",
    "\n",
    "    try:\n",
    "        paper = next(search_result)\n",
    "    except StopIteration:\n",
    "        raise ValueError(f\"Paper not found for ArXiv ID: {arxiv_id}\")\n",
    "\n",
    "    # Download PDF and extract text content\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        pdf_path = paper.download_pdf(dirpath=temp_dir)\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\".join([page.get_text() for page in doc])\n",
    "\n",
    "    return ArXivPaper(title=paper.title, text=text, url=url)\n",
    "\n",
    "\n",
    "def fetch_multiple_papers(urls: list[str]) -> list[ArXivPaper]:\n",
    "    \"\"\"Fetches and downloads multiple ArXiv papers from the given URLs and returns a list of ArXivPaper objects.\"\"\"\n",
    "    return [fetch_single_paper(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval' text='Published as a conference paper at ICLR 2024\\nRAPTOR: RECURSIVE ABSTRACTIVE PROCESSING\\nFOR TREE-ORGANIZED RETRIEVAL\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\\nStanford University\\npsarthi@cs.stanford.edu\\nABSTRACT\\nRetrieval-augmented language models can better adapt to changes in world state\\nand incorporate long-tail knowledge. However, most existing methods retrieve\\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\\nstanding of the overall document context. We introduce the novel approach of\\nrecursively embedding, clustering, and summarizing chunks of text, constructing\\na tree with differing levels of summarization from the bottom up. At inference\\ntime, our RAPTOR model retrieves from this tree, integrating information across\\nlengthy documents at different levels of abstraction. Controlled experiments show\\nthat retrieval with recursive summaries offers significant improvements over tra-\\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\\n1\\nINTRODUCTION\\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\\nmance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream\\ntasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-\\nspecific knowledge for particular tasks and the world continues to change, invalidating facts in the\\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\\nparticularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).\\nNevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\\nan entire book, as in the NarrativeQA dataset (Koˇ\\ncisk`\\ny et al., 2018). Consider the fairy tale of\\nCinderella, and the question “How did Cinderella reach her happy ending?”. The top-k retrieved\\nshort contiguous texts will not contain enough context to answer the question.\\nTo address this, we design an indexing and retrieval system that uses a tree structure to capture both\\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\\nthe bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing\\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\\n1\\narXiv:2401.18059v1  [cs.CL]  31 Jan 2024\\nPublished as a conference paper at ICLR 2024\\n2\\n3\\n4\\n5\\n1\\n1\\n2\\n3\\n3\\n4\\n5\\n5\\n6\\n8\\n7\\nIndex #8\\nText:  summary of \\nnodes 2 and 3\\nChild Nodes: 2, 3\\nText Embedding\\nText chunks\\n3\\n.1\\n4\\n.1\\n5\\n2. Summarization \\nby LLM\\n1. Clustering\\n10\\n7\\n1\\n2\\n8\\n4\\n3\\n5\\n6\\n9\\nFormation of one tree layer\\nRoot layer\\nLeaf layer\\nContents of a node\\nRAPTOR Tree \\nFigure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their\\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\\ncluster.\\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\\ncontext at different scales, and to show its effectiveness in experiments on collections of long doc-\\numents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-\\nfiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\\nand movies (NarrativeQA, Koˇ\\ncisk`\\ny et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\\nand multiple-choice questions based on medium-length passages (QuALITY, Pang et al. 2022).1\\n2\\nRELATED WORK\\nWhy Retrieval?\\nRecent advances in hardware and algorithms have indeed expanded the con-\\ntext lengths that models can handle, leading to questions about the need for retrieval systems (Dai\\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\\nhave noted, models tend to underutilize long-range context and see diminishing performance as con-\\ntext length increases, especially when pertinent information is embedded within a lengthy context.\\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\\nmost relevant information for knowledge-intensive tasks is still crucial.\\nRetrieval Methods\\nRetrieval-augmented language models (RALMs) have seen improvements in\\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\\nhave transitioned from traditional term-based techniques like TF-IDF (Sp¨\\narck Jones, 1972) and\\nBM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin\\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\\n2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)\\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\\nindependently in the encoder and RETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\\nEnd-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\\ndecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked\\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-\\ndecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-\\nerarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements\\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\\n1We will release the code of RAPTOR publicly here.\\n2\\nPublished as a conference paper at ICLR 2024\\nDespite a diversity in methods, the retrieving components of models predominantly rely on stan-\\ndard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this\\napproach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous seg-\\nmentation might not capture the complete semantic depth of the text. Reading extracted snippets\\nfrom technical or scientific documents may lack important context making them difficult to read or\\neven misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).\\nRecursive summarization as Context\\nSummarization techniques provide a condensed view of\\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both\\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\\noverlook distant interdependencies within the text, which we can find and group with RAPTOR.\\n3\\nMETHODS\\nOverview of RAPTOR\\nBuilding on the idea that long texts often present subtopics and hierarchi-\\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\\nilarity not just order in the text.\\nConstruction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\\ntexts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the\\n100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence.\\nThis preserves the contextual and semantic coherence of the text within each chunk. These texts\\nare then embedded using SBERT, a BERT-based encoder (multi-qa-mpnet-base-cos-v1)\\n(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\\nleaf nodes of our tree structure.\\nTo group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\\nresulting in a structured, multi-layered tree representation of the original documents. An important\\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A.\\nFor querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\\nthe most relevant ones.\\nClustering Algorithm\\nClustering plays a key role in building the RAPTOR tree, organizing text\\nsegments into cohesive groups. This step groups related content together, which helps the subse-\\nquent retrieval process.\\nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\\ntial because individual text segments often contain information relevant to various topics, thereby\\nwarranting their inclusion in multiple summaries.\\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\\nmixture of several Gaussian distributions.\\n3\\nPublished as a conference paper at ICLR 2024\\nGiven a set of N text segments, each represented as a d-dimensional dense vector embedding, the\\nlikelihood of a text vector, x, given its membership in the kth Gaussian distribution, is denoted by\\nP(x|k) = N(x; µk, Σk). The overall probability distribution is a weighted combination P(x) =\\nPK\\nk=1 πkN(x; µk, Σk), where πk signifies the mixture weight for the kth Gaussian distribution.\\nThe high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\\ntance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\\ngarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\\nnumber of nearest neighbors parameter, n neighbors, in UMAP determines the balance between\\nthe preservation of local and global structures. Our algorithm varies n neighbors to create a hierar-\\nchical clustering structure: it first identifies global clusters and then performs local clustering within\\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\\namong the text data, from broad themes to specific details.\\nShould a local cluster’s combined context ever exceed the summarization model’s token threshold,\\nour algorithm recursively applies clustering within the cluster, ensuring that the context remains\\nwithin the token threshold.\\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N)k −2 ln(ˆ\\nL), where N is the number\\nof text segments (or data points), k is the number of model parameters, and ˆ\\nL is the maximized\\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k\\nis a function of the dimensionality of the input vectors and the number of clusters.\\nWith the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\\nis then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\\nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\\ncontiguous chunks and provide details in Appendix B.\\nModel-Based Summarization\\nAfter clustering the nodes using Gaussian Mixture Models, the\\nnodes in each cluster are sent to a language model for summarization. This step allows the model\\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\\ndenses the potentially large volume of retrieved information into a manageable size. We provide\\nstatistics on the compression due to the summarization in Appendix C and the prompt used for\\nsummarization in Appendix D.\\nWhile the summarization model generally produces reliable summaries, a focused annotation study\\nrevealed that about 4% of the summaries contained minor hallucinations. These did not propagate\\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\\nof hallucinations, refer to the appendix E.\\nQuerying\\nIn this section, we elaborate on the two querying mechanisms employed by RAPTOR:\\ntree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\\nRAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\\nprovide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\\nThe tree traversal method first selects the top-k most relevant root nodes based on their cosine\\nsimilarity to the query embedding. The children of these selected nodes are considered at the next\\nlayer and the top-k nodes are selected from this pool again based on their cosine similarity to the\\nquery vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\\nnodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:\\n1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\\nquery embedding and the embeddings of all nodes present at this initial layer.\\n2. Choose the top-k nodes based on the highest cosine similarity scores, forming the set S1.\\n4\\nPublished as a conference paper at ICLR 2024\\nFigure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The\\nnodes on which cosine similarity search is performed are highlighted in both illustrations.\\n3. Proceed to the child nodes of the elements in set S1. Compute the cosine similarity between\\nthe query vector and the vector embeddings of these child nodes.\\n4. Select the top k child nodes with the highest cosine similarity scores to the query, forming\\nthe set S2.\\n5. Continue this process recursively for d layers, producing sets S1, S2, . . . , Sd.\\n6. Concatenate sets S1 through Sd to assemble the relevant context to the query.\\nBy adjusting the depth d and the number of nodes k selected at each layer, the tree traversal method\\noffers control over the specificity and breadth of the information retrieved. The algorithm starts with\\na broad outlook by considering the top layers of the tree and progressively focuses on finer details\\nas it descends through the lower layers.\\nThe collapsed tree approach offers a simpler way to search for relevant information by considering\\nall nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\\nmethod flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\\nsame level for comparison. The steps for this method are outlined below:\\n1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\\nas C, contains nodes from every layer of the original tree.\\n2. Next, calculate the cosine similarity between the query embedding and the embeddings of\\nall nodes present in the collapsed set C.\\n3. Finally, pick the top-k nodes that have the highest cosine similarity scores with the query.\\nKeep adding nodes to the result set until you reach a predefined maximum number of\\ntokens, ensuring you don’t exceed the model’s input limitations.\\nWe tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\\nof tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\\nThe collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\\nbetter due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\\nsimultaneously, it retrieves information that is at the correct level of granularity for a given question.\\nIn comparison, while using tree traversal with the same values of d and k, the ratio of nodes from\\neach level of the tree will be constant. So, the ratio of higher-order thematic information to granular\\ndetails will remain the same regardless of the question.\\n5\\nPublished as a conference paper at ICLR 2024\\nOne drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\\nbe performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest\\nneighbor libraries such as FAISS (Johnson et al., 2019).\\nFigure 3: Comparison of querying methods.\\nResults on 20 stories from the QASPER dataset\\nusing tree traversal with different top-k values,\\nand collapsed tree with different context lengths.\\nCollapsed tree with 2000 tokens produces the best\\nresults, so we use this querying strategy for our\\nmain results.\\nOverall, given the collapsed tree approach’s\\ngreater flexibility and its superior performance\\non the subset of the QASPER dataset, this is\\nthe querying approach with which we proceed.\\nSpecifically, we use the collapsed tree with\\n2000 maximum tokens, which approximately\\nequates to retrieving the top-20 nodes. Using a\\ntoken-based approach ensures the context does\\nnot exceed model context constraints as token\\ncounts can vary across nodes. For experiments\\nwith the UnifiedQA model, we provide 400 to-\\nkens of context, as UnifiedQA has a max con-\\ntext length of 512 tokens. We provide the same\\namount of tokens of context to RAPTOR and to\\nthe baselines.\\nQualitative Study\\nWe conduct a qualitative\\nanalysis to understand the benefits of RAP-\\nTOR’s retrieval process compared to Dense\\nPassage Retrieval (DPR) methods. Our study\\nfocuses on thematic, multi-hop questions using\\na 1500-word Cinderella fairytale. As illustrated\\nin Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers,\\nmatching the question’s detail level. This approach often yields more relevant and comprehensive\\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\\n4\\nEXPERIMENTS\\nDatasets\\nWe measure RAPTOR’s performance across three question-answering datasets: Narra-\\ntiveQA, QASPER, and QuALITY.\\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\\nand movie transcripts, totaling 1,572 documents (Koˇ\\ncisk`\\ny et al., 2018; Wu et al., 2021).\\nThe\\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\\nto accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\\ntiveQA evaluation script used in our experiments.\\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\\nare categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\\nmeasured using standard F1.\\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our re-\\ntrieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-\\nHARD, which contains questions that a majority of human annotators answered incorrectly in a\\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\\nControlled Baseline Comparisons\\nWe first present controlled comparisons using the UnifiedQA\\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY. As shown in Tables 1 and 2,\\n6\\nPublished as a conference paper at ICLR 2024\\nFigure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions\\nabout the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find\\na happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s\\nleaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\\ndirectly or within higher-layer summaries.\\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\\nthe respective retriever across all datasets. 2\\nSince RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\\nWe now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\\nUnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\\nthree Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are 53.1%, 55.7%,\\nand 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\\nmargins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\\nLLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\\nRAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract\\nthe top-k most similar raw chunks of text, which may not contain the correct response in isolation.\\nTable 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\\nspective retrieval method.\\nModel\\nROUGE\\nBLEU-1\\nBLEU-4\\nMETEOR\\nSBERT with RAPTOR\\n30.87%\\n23.50%\\n6.42%\\n19.20%\\nSBERT without RAPTOR\\n29.26%\\n22.56%\\n5.95%\\n18.15%\\nBM25 with RAPTOR\\n27.93%\\n21.17%\\n5.70%\\n17.03%\\nBM25 without RAPTOR\\n23.52%\\n17.73%\\n4.65%\\n13.98%\\nDPR with RAPTOR\\n30.94%\\n23.51%\\n6.45%\\n19.05%\\nDPR without RAPTOR\\n29.56%\\n22.84%\\n6.12%\\n18.44%\\nLikewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\\nwhich is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-\\nfiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\\nFinally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-\\nrics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\\nmetrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\\n7\\nPublished as a conference paper at ICLR 2024\\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\\nforms baselines of each respective retrieval method for both datasets.\\nModel\\nAccuracy (QuALITY)\\nAnswer F1 (QASPER)\\nSBERT with RAPTOR\\n56.6%\\n36.70%\\nSBERT without RAPTOR\\n54.9%\\n36.23%\\nBM25 with RAPTOR\\n52.1%\\n27.00%\\nBM25 without RAPTOR\\n49.9%\\n26.47%\\nDPR with RAPTOR\\n54.7%\\n32.23%\\nDPR without RAPTOR\\n53.1%\\n31.70%\\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-\\nguage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title +\\nAbstract” reflects performance when only the title and abstract of the papers are used for context.\\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\\nSpecifically, RAPTOR’s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\\nhigher than BM25.\\nRetriever\\nGPT-3 F-1 Match\\nGPT-4 F-1 Match\\nUnifiedQA F-1 Match\\nTitle + Abstract\\n25.2\\n22.2\\n17.5\\nBM25\\n46.6\\n50.2\\n26.4\\nDPR\\n51.3\\n53.0\\n32.1\\nRAPTOR\\n53.1\\n55.7\\n36.6\\nTable 4: Comparison of accuracies on the QuAL-\\nITY dev dataset for two different language mod-\\nels (GPT-3, UnifiedQA 3B) using various retrieval\\nmethods. RAPTOR outperforms the baselines of\\nBM25 and DPR by at least 2.0% in accuracy.\\nModel\\nGPT-3 Acc.\\nUnifiedQA Acc.\\nBM25\\n57.3\\n49.9\\nDPR\\n60.4\\n53.9\\nRAPTOR\\n62.4\\n56.6\\nTable 5: Results on F-1 Match scores of various\\nmodels on the QASPER dataset.\\nModel\\nF-1 Match\\nLongT5 XL (Guo et al., 2022)\\n53.1\\nCoLT5 XL (Ainslie et al., 2023)\\n53.9\\nRAPTOR + GPT-4\\n55.7\\nComparison\\nto\\nState-of-the-art\\nSystems\\nBuilding upon our controlled comparisons,\\nwe examine RAPTOR’s performance relative\\nto other state-of-the-art models.\\nAs shown\\nin Table 5, RAPTOR with GPT-4 sets a new\\nbenchmark on QASPER, with a 55.7% F-1\\nscore, surpassing the CoLT5 XL’s score of\\n53.9%.\\nIn the QuALITY dataset, as shown in Table 7,\\nRAPTOR paired with GPT-4 sets a new state-\\nof-the-art with an accuracy of 82.6%, surpass-\\ning the previous best result of 62.3%. In par-\\nticular, it outperforms CoLISA by 21.5% on\\nQuALITY-HARD, which represents questions\\nthat humans took unusually long to correctly\\nanswer, requiring rereading parts of the text,\\ndifficult reasoning, or both.\\nFor the NarrativeQA dataset, as represented in\\nTable 6, RAPTOR paired with UnifiedQA sets\\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by\\nWu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\\nWu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\\nbenefits from its intermediate layers and clustering approaches, which allows it to capture a range of\\ninformation, from general themes to specific details, contributing to its overall strong performance.\\n4.1\\nCONTRIBUTION OF THE TREE STRUCTURE\\nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy-\\npothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\\na broader understanding of the text.\\n8\\nPublished as a conference paper at ICLR 2024\\nTable 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\\non four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-\\nfiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-\\nthe-art in the METEOR metric.\\nModel\\nROUGE-L\\nBLEU-1\\nBLEU-4\\nMETEOR\\nBiDAF (Koˇ\\ncisk`\\ny et al., 2018)\\n6.2\\n5.7\\n0.3\\n3.7\\nBM25 + BERT (Mou et al., 2020)\\n15.5\\n14.5\\n1.4\\n5.0\\nRecursively Summarizing Books (Wu et al., 2021)\\n21.6\\n22.3\\n4.2\\n10.6\\nRetriever + Reader (Izacard & Grave, 2022)\\n32.0\\n35.3\\n7.5\\n11.1\\nRAPTOR + UnifiedQA\\n30.8\\n23.5\\n6.4\\n19.1\\nTable 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\\nhard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\\nModel\\nAccuracy\\nTest Set\\nHard Subset\\nLongformer-base (Beltagy et al., 2020)\\n39.5\\n35.3\\nDPR and DeBERTaV3-large (Pang et al., 2022)\\n55.4\\n46.1\\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a)\\n62.3\\n54.7\\nRAPTOR + GPT-4\\n82.6\\n76.2\\nTable 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-\\nITY dataset. Columns represent different starting points (highest layer) and rows represent different\\nnumbers of layers queried.\\nLayers Queried / Start Layer\\nLayer 0 (Leaf Nodes)\\nLayer 1\\nLayer 2\\n1 layer\\n57.9\\n57.8\\n57.9\\n2 layers\\n-\\n52.6\\n63.15\\n3 layers\\n-\\n-\\n73.68\\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\\nappendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\\nfrom the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\\nwe exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\\nsubsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\\nsearch, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\\nThese findings highlight the importance of the full tree structure in RAPTOR. By providing both\\nthe original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\\nrange of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\\nfor additional stories and an ablation study on layer contributions can be found in Appendix I.\\n5\\nCONCLUSION\\nIn this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\\nparametric knowledge of large language models with contextual information at various levels of\\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\\nhierarchical tree structure that is capable of synthesizing information across various sections of the\\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.\\n9\\nPublished as a conference paper at ICLR 2024\\n6\\nREPRODUCIBILITY STATEMENT\\nLanguage Models for QA and Summarization\\nFour language models are used in our RAPTOR\\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\\ngpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\\nwhich is used for QA tasks, is publicly available at Hugging Face.\\nEvaluation Datasets\\nThe three evaluation datasets used in our experiments—QuALITY,\\nQASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval\\nand QA tests conducted in this study can be replicated.\\nSource Code\\nThe source code for RAPTOR will be publicly available here.\\nREFERENCES\\nCharu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-\\ntance Metrics in High Dimensional Space. In Database Theory—ICDT 2001: 8th International\\nConference London, UK, January 4–6, 2001 Proceedings 8, pp. 420–434. Springer, 2001. URL\\nhttps://link.springer.com/chapter/10.1007/3-540-44503-x_27.\\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta˜\\nn´\\non, Siddhartha Brahma, Yury Zemlyan-\\nskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al.\\nCoLT5: Faster long-range\\ntransformers with conditional computation.\\narXiv preprint arXiv:2303.09752, 2023.\\nURL\\nhttps://arxiv.org/abs/2303.09752.\\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\\nKelvin Guu.\\nTowards tracing knowledge in language models back to the training data.\\nIn\\nFindings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429–2446,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\\nfindings-emnlp.180.\\nStefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\\nprediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858, 2018. URL\\nhttps://arxiv.org/abs/1808.08858.\\nManoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\\nHuang.\\nHybrid hierarchical retrieval for open-domain question answering.\\nIn Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational\\nLinguistics: ACL 2023, pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.\\norg/2023.findings-acl.679.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\\n2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\\nImproving language models by retrieving from trillions of tokens. In International conference on\\nmachine learning, pp. 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.\\n04426.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\\nford, Ilya Sutskever, and Dario Amodei.\\nLanguage Models are Few-Shot Learners.\\nIn\\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\\nral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,\\n10\\nPublished as a conference paper at ICLR 2024\\n2020.\\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nS´\\nebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\\nIntelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL\\nhttps://arxiv.org/abs/2303.12712.\\nShuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\\ndocument summarization. In Proceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 786–807, Dublin, Ireland, May 2022.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:\\n//aclanthology.org/2022.acl-long.58.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.\\nReading Wikipedia to Answer\\nOpen-Domain Questions.\\nIn Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 1870–1879, Vancouver, Canada, July\\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\\n//aclanthology.org/P17-1171.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\\nScaling Language Modeling with Pathways.\\narXiv preprint arXiv:2204.02311, 2022.\\nURL\\nhttps://arxiv.org/abs/2204.02311.\\nArman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\\nword embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR\\nConference on Research and Development in Information Retrieval, pp. 1133–1136, 2017. URL\\nhttps://dl.acm.org/doi/abs/10.1145/3077136.3080740.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\\nhttps://aclanthology.org/P19-1285.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´\\ne.\\nFlashAttention: Fast and\\nmemory-efficient exact attention with IO-Awareness. Advances in Neural Information Processing\\nSystems, 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\\nof Information-Seeking Questions and Answers Anchored in Research Papers.\\nIn Proceed-\\nings of the 2021 Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, pp. 4599–4610, Online, June 2021. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:\\n//aclanthology.org/2021.naacl-main.365.\\nMengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\\nLearning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th\\nEuropean Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023,\\nProceedings, Part I, pp. 264–278. Springer, 2023a. URL https://link.springer.com/\\nchapter/10.1007/978-3-031-28244-7_17.\\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\\ntransformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/\\n2302.14502.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/\\nabs/2305.14627.\\n11\\nPublished as a conference paper at ICLR 2024\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, pp. 724–736, Seattle, United States,\\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\\nURL https://aclanthology.org/2022.findings-naacl.55.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\\nLanguage Model Pre-Training. In International conference on machine learning, pp. 3929–3938.\\nPMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL\\nhttps://arxiv.org/abs/2203.15556.\\nGautier Izacard and Edouard Grave.\\nDistilling Knowledge from Reader to Retriever for Ques-\\ntion Answering, 2022.\\nURL https://arxiv.org/abs/2012.04584.\\narXiv preprint\\narXiv:2012.04584.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https:\\n//arxiv.org/abs/2208.03299.\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\\nmodels know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\\nURL https://arxiv.org/abs/1911.12543.\\nJeff Johnson, Matthijs Douze, and Herv´\\ne J´\\negou. Billion-Scale Similarity Search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\\n08734.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\\ning, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\\nkandpal23a/kandpal23a.pdf.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), pp. 6769–6781, Online, November 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\\nemnlp-main.550.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\\nHannaneh Hajishirzi.\\nUNIFIEDQA: Crossing format boundaries with a single QA system.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907,\\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\\nfindings-emnlp.171.\\nURL https://aclanthology.org/2020.findings-emnlp.\\n171.\\nOmar Khattab and Matei Zaharia.\\nColBERT: Efficient and effective passage search via con-\\ntextualized late interaction over bert.\\nIn Proceedings of the 43rd International ACM SIGIR\\nconference on research and development in Information Retrieval, pp. 39–48, 2020.\\nURL\\nhttps://arxiv.org/abs/2004.12832.\\nTom´\\naˇ\\ns Koˇ\\ncisk`\\ny, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´\\nabor Melis,\\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\\nof the Association for Computational Linguistics, 6:317–328, 2018. URL https://arxiv.\\norg/abs/1712.07040.\\n12\\nPublished as a conference paper at ICLR 2024\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K¨\\nuttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨\\naschel, et al. Retrieval-Augmented Gener-\\nation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems,\\n33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.\\nJerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang.\\nLost in the middle: How language models use long contexts.\\narXiv preprint\\narXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\\nhierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\\nHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2021, pp. 188–200, Punta Cana, Dominican Republic, Novem-\\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\\nURL https://aclanthology.org/2021.findings-emnlp.19.\\nLeland McInnes, John Healy, and James Melville.\\nUMAP: Uniform Manifold Approximation\\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\\n03426. arXiv preprint arXiv:1802.03426.\\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\\npassage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, pp. 6997–7008, Online and Punta Cana, Dominican\\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\nemnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.\\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\\nZettlemoyer.\\nNonparametric masked language modeling.\\nIn Findings of the Association for\\nComputational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\\n//aclanthology.org/2023.findings-acl.132.\\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\\nMemory-based model editing at scale.\\nIn International Conference on Machine Learning,\\npp. 15817–15831. PMLR, 2022.\\nURL https://proceedings.mlr.press/v162/\\nmitchell22a/mitchell22a.pdf.\\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\\nSu. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\\nWorkshop on Narrative Understanding, Storylines, and Events, pp. 108–113, Online, July 2020.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\\n//aclanthology.org/2020.nuse-1.13.\\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\\nishna Karanam, and Sumit Shekhar.\\nA neural CRF-based hierarchical approach for lin-\\near text segmentation.\\nIn Findings of the Association for Computational Linguistics: EACL\\n2023, pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\\nfindings-eacl.65.\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\\nbased framework for decontextualization. arXiv preprint arXiv:2305.14772, 2023. URL https:\\n//arxiv.org/pdf/2305.14772.pdf.\\nOpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/\\nabs/2303.08774.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\\n13\\nPublished as a conference paper at ICLR 2024\\nQuestion Answering with Long Input Texts, Yes!\\nIn Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\\nLinguistics. URL https://aclanthology.org/2022.naacl-main.391.\\nFabio Petroni, Tim Rockt¨\\naschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\\n2019. URL https://arxiv.org/abs/1909.01066.\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021.\\nURL https://arxiv.org/abs/2112.11446.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\\nBrown, and Yoav Shoham.\\nIn-context retrieval-augmented language models.\\narXiv preprint\\narXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\\nD19-1410.\\nAdam Roberts, Colin Raffel, and Noam Shazeer.\\nHow Much Knowledge Can You Pack Into\\nthe Parameters of a Language Model?\\nIn Proceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, Online, November\\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\\nhttps://aclanthology.org/2020.emnlp-main.437.\\nStephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\\nBeyond. Foundations and Trends in Information Retrieval, 3(4):333–389, 2009. URL https:\\n//doi.org/10.1561/1500000019.\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\\net al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.\\nmicrosoft.com/en-us/research/publication/okapi-at-trec-3/.\\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\\nZaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\\nsociation for Computational Linguistics, 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL\\nhttps://aclanthology.org/2023.tacl-1.35.\\nGideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461–464,\\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\\naos/1176344136.full.\\nKaren Sp¨\\narck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\\ntrieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/\\neb026526.\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\\nmodels actually use long-range context?\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia\\nSpecia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 807–822, Online and Punta Cana, Dominican Republic,\\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\\n62. URL https://aclanthology.org/2021.emnlp-main.62.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\\nmodels. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.\\n01296.\\n14\\nPublished as a conference paper at ICLR 2024\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language\\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\\n743–758, 2020. URL https://arxiv.org/abs/1912.13283.\\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:\\n//arxiv.org/abs/2304.06762.\\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\\nChristiano.\\nRecursively Summarizing Books with Human Feedback, 2021.\\nURL https:\\n//arxiv.org/abs/2109.10862.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\\nand Quoc V. Le. QANet: Combining Local Convolution with Global Self-Attention for Read-\\ning Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint\\narXiv:1804.09541.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang\\nZhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are\\nstrong context generators, 2022. URL https://arxiv.org/abs/2209.10063.\\nShiyue Zhang, David Wan, and Mohit Bansal.\\nExtractive is not faithful: An investigation of\\nbroad unfaithfulness problems in extractive summarization.\\nIn Anna Rogers, Jordan Boyd-\\nGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2153–2174, Toronto, Canada, July\\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL\\nhttps://aclanthology.org/2023.acl-long.120.\\nA\\nSCALABILITY AND COMPUTATIONAL EFFICIENCY OF THE\\nTREE-BUILDING PROCESS\\nTo assess the computational efficiency and cost-effectiveness of RAPTOR’s tree-building process,\\nwe conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB\\nof RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on\\ntypical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the\\ntoken expenditure and the time required to complete the tree-building process, from initial splitting\\nand embedding to the construction of the final root node.\\nFigure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY.\\nRAPTOR tree construction costs scale linearly with document length for each of the datasets.\\nToken Expenditure\\nWe empirically investigated the relationship between the initial document\\nlength and the total number of tokens expended during the tree-building process, which includes\\nboth the prompt and completion tokens. The document lengths varied significantly across the three\\n15\\nPublished as a conference paper at ICLR 2024\\ndatasets examined: QuALITY, QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-\\nlation between the initial document length and the total token expenditure, emphasizing that RAP-\\nTOR maintains a linear token scaling regardless of document complexity or length.\\nFigure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-\\nTOR tree construction time scales linearly with document length for each of the datasets.\\nBuild Time\\nWe also empirically observed a consistent linear trend between the document length\\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\\nConclusion\\nOverall, our empirical results indicate that RAPTOR scales both in terms of tokens\\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-\\ntionally efficient and well-suited for processing large and diverse corpora.\\nB\\nABLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\\nan ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a\\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\\nclustering method.\\nB.1\\nMETHODOLOGY\\nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain\\nconsistency in retrieval. For RAPTOR, we employed our typical clustering and summarization\\nprocess. In contrast, the alternative setup involved creating a balanced tree by recursively encoding\\nand summarizing contiguous text chunks. We determined the window size for this setup based on\\nthe average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose\\na window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.\\nB.2\\nRESULTS & DISCUSSION\\nThe results of the ablation study are presented in table 9. The results from this ablation study clearly\\nindicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the\\nrecency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in\\nRAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing\\nthe overall retrieval performance.\\n16\\nPublished as a conference paper at ICLR 2024\\nTable 9: Ablation study results comparing RAPTOR with a recency-based tree approach\\nConfiguration\\nAccuracy\\nRAPTOR + SBERT embeddings + UnifiedQA\\n56.6%\\nRecency-based tree + SBERT embeddings + UnifiedQA\\n55.8%\\nC\\nDATASET STATISTICS AND COMPRESSION RATIOS\\nThe average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,\\nindicating a 72% compression rate. On average, the summary length is 131 tokens, and the average\\nchild node length is 86 tokens. Below are the detailed statistics for all three datasets:\\nTable 10: Statistics of Average Summary Length and Child Node Length Across Datasets\\nDataset\\nAvg.\\nSummary\\nLength\\n(tokens)\\nAvg. Child\\nNode Text\\nLength\\n(tokens)\\nAvg. # of\\nChild Nodes\\nPer Parent\\nAvg.\\nCompression\\nRatio (%)\\nAll Datasets\\n131\\n85.6\\n6.7\\n.28\\nQuALITY\\n124.4\\n87.9\\n5.7\\n.28\\nNarrativeQA\\n129.7\\n85.5\\n6.8\\n.27\\nQASPER\\n145.9\\n86.2\\n5.7\\n.35\\nD\\nSUMMARIZATION PROMPT\\nTable 11 shows the prompt used for summarization.\\nTable 11: Prompt for Summarization\\nRole\\nContent\\nsystem\\nYou are a Summarizing Text Portal\\nuser\\nWrite a summary of the following, including as many key details as\\npossible: {context}:\\nE\\nHALLUCINATION ANALYSIS\\nTo assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted\\nan analysis focusing on hallucinations in the generated summaries. The summaries were generated\\nby gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine\\nwhether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-\\nanswering (QA) tasks.\\nE.1\\nMETHODOLOGY\\nWe randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This\\nsampling strategy provides a broad view of the model’s performance across different contexts. Each\\nnode was annotated by hand, and determined if it contained a hallucination.\\nE.2\\nFINDINGS\\nOut of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,\\nthese hallucinations originated from the model adding minor information possibly from its training\\ndata that was not present in the text being summarized, or from incorrectly extrapolating some\\ninformation when creating the summary.\\n17\\nPublished as a conference paper at ICLR 2024\\nExample:\\nText of the child nodes:\\n”And you will come with me to my people? We may live here among them, and\\nyou will be a great warrior–oh, when Jor dies you may even be chief, for there is\\nnone so mighty as my warrior...”But your father will not permit it–Jor, my father,\\nHigh Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co-\\nTan, if we but could!... Bradley noticed that she spoke in English–broken English\\nlike Co-Tan’s but equally appealing.\\nSummary found in the parent of that node:\\nThe protagonist, Bradley, is being asked by Co-Tan to stay with her people and\\nbecome a great warrior, but he refuses and must return to his own country. Tom\\nBillings of Santa Monica arrives and tells them he came to search for a man named\\nBowen J. Tyler, Jr. Ajor, Co-Tan’s sister, is excited about the possibility of going\\nto Tom’s country to see strange and wonderful things...\\nThe hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not\\nexplicitly mention or imply this.\\nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.\\nGenerally, the hallucinations were minor and did not alter the thematic interpretation of the text.\\nE.3\\nIMPACT ON QA TASKS\\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\\narchitecture.\\nF\\nPSEUDOCODE FOR RETRIEVAL METHODS\\nAlgorithm 1 Tree Traversal Algorithm\\nfunction TRAVERSETREE(tree, query, k)\\nScurrent ←tree.layer[0]\\nfor layer in range(tree.num layers) do\\ntopk ←[]\\nfor node in Scurrent do\\nscore ←dot product(query, node)\\ntop k.append((node, score))\\nend for\\nSlayer ←sorted(top k)[:k].nodes\\nScurrent ←Slayer\\nend for\\nreturn S0 ∪S1 ∪S2 ∪. . . ∪Sk\\nend function\\nG\\nQUALITATIVE ANALYSIS\\nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions\\nabout a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP-\\nTOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details\\nthe retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR\\nselects for each question are highlighted, while the leaf nodes that DPR selects for the same question\\nare indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure.\\nRAPTOR selects nodes from different layers depending on the level of granularity required by the\\n18\\nPublished as a conference paper at ICLR 2024\\nAlgorithm 2 Collapsed Tree Algorithm\\nfunction COLLAPSEDTREE(tree, query, k, max tokens)\\ntree ←flatten(tree)\\n▷Flatten tree into 1D\\ntop nodes ←[]\\nfor node in tree do\\ntop nodes.append((node, dot product(query, node))\\nend for\\ntop nodes ←sorted(top nodes)\\nresult ←[]\\ntotal tokens ←0\\nfor node in top nodes do\\nif total tokens + node.token size < max tokens then\\nresult.append(node)\\nend if\\ntotal tokens ←total tokens + node.token size\\nend for\\nreturn result\\nend function\\nQuestion: What is the central theme of the story?\\nRAPTOR\\nFairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR\\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR\\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR\\nthe clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . .\\nFairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not\\nincluded in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a\\nsummary from a higher layer.\\n”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question\\nbest answered by synthesizing information from various text segments. To control for the language\\nmodel’s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved\\ninformation for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this\\nquestion. RAPTOR’s context succinctly describes Cinderella’s journey to happiness, while DPR’s\\nleaf nodes primarily focus on her initial transformation. The difference in retrieved information\\n19\\nPublished as a conference paper at ICLR 2024\\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen-\\nerates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner\\nof the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-\\ning Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on the\\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\\ninformation about the story’s conclusion.”\\nThe second question we examine is “What is the central theme of the story?”, a thematic question\\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\\nof GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of\\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\\nand others at the ball.” This answer only takes into account the first portion of the story, up until\\nCinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central\\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\\nyoung woman who ultimately finds happiness and love with the Prince.” This is a more complete\\nanswer, demonstrating a comprehensive understanding of the story.\\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\\ndownstream tasks.\\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\\nfrom the story is present below and the full PDF of this story is linked here. For questions like “What\\nis the central theme of the story?”, an upper-level node is retrieved which includes the sentence:\\n“This story is about the power of human connection... inspiring and uplifting each other as they\\npursued their passions.” This summary, not explicitly present in the original text, almost directly\\nanswers the question.\\nExcerpt from ”The Eager Writer”:\\n”Ethan’s passion for writing had always been a part of him. As a child, he would\\noften scribble stories and poems in his notebook, and as he grew older, his love\\nfor writing only intensified. His evenings were often spent in the dim light of his\\nroom, typing away at his laptop. He had recently taken a job as a content writer\\nfor an online marketing firm to pay the bills, but his heart still longed for the\\nworld of storytelling. However, like many aspiring writers, he struggled to find a\\nfoothold in the industry. He took a job as a content writer for an online marketing\\nfirm, but it was growing increasingly evident to him that this was not the path he\\nwanted to pursue. It was during this time that he stumbled upon the Pathways\\napp. The app offered a platform for people in similar professions to connect and\\nshare knowledge, and he saw it as an opportunity to finally connect with others\\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\\nshared his passion and could offer guidance and mentorship. He quickly signed\\nup and was surprised by the number of writers he found on the platform, from\\nwell establish professionals to beginners just starting out in the business.”\\nH\\nNARRATIVEQA EVALUATION SCRIPT\\nWe made several modifications to AllenNLP’s evaluation script3 to better fit our evaluation needs:\\n• Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is\\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\\n3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\\n20\\nPublished as a conference paper at ICLR 2024\\na smoothing function, we prevent the BLEU scores from dropping to zero, providing a more\\nfair evaluation.\\n• Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest\\norder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\\n0.25).\\n• Tokenization before Mapping in METEOR Calculation: The original script utilized a\\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\\ncalculation by taking into account the correct linguistic boundaries of words.\\nQuestion: What is the central theme of the story?\\nRAPTOR\\nFairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR\\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR\\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR\\nthe clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . .\\nFairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nI\\nANALYSIS OF DIFFERENT LAYERS ON RAPTOR’S PERFORMANCE\\nI.1\\nHOW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\\nIn this section, we present a detailed breakdown of RAPTOR’s retrieval performance when querying\\ndifferent layers of the hierarchical tree structure for various stories. These tables validate the utility\\nof RAPTOR’s multi-layered structure for diverse query requirements.\\nTable 14: Performance of RAPTOR when querying different layers of the tree for Story 2.\\nLayers Queried / Start Layer\\nLayer 0 (Leaf Nodes)\\nLayer 1\\nLayer 2\\n1 layer\\n58.8\\n47.1\\n41.1\\n2 layers\\n-\\n64.7\\n52.9\\n3 layers\\n-\\n-\\n47.1\\n21\\nPublished as a conference paper at ICLR 2024\\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR\\ntree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,\\nand DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval\\ncomes from non-leaf layers, with a notable percentage from the first and second layers, highlighting\\nthe importance of RAPTOR’s hierarchical summarization in the retrieval process.\\nTable 15: Performance of RAPTOR when querying different layers of the tree for Story 3.\\nLayers Queried / Start Layer\\nLayer 0 (Leaf Nodes)\\nLayer 1\\nLayer 2\\n1 layer\\n66.6\\n61.1\\n61.1\\n2 layers\\n-\\n66.6\\n66.6\\n3 layers\\n-\\n-\\n83.3\\nTable 16: Performance of RAPTOR when querying different layers of the tree for Story 4.\\nLayers Queried / Start Layer\\nLayer 0 (Leaf Nodes)\\nLayer 1\\n1 layer\\n94.7\\n84.2\\n2 layers\\n-\\n89.4\\nTable 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\\nLayers Queried / Start Layer\\nLayer 0 (Leaf Nodes)\\nLayer 1\\n1 layer\\n57.9\\n47.3\\n2 layers\\n-\\n68.4\\nI.2\\nWHICH LAYERS DO RETRIEVED NODES COME FROM ?\\nWe further conduct an ablation study across all three datasets and across three different retrievers\\nwith RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes\\noriginate. We observe that between 18.5% to 57% of the retrieved nodes come from non-leaf nodes.\\nAs illustrated in Figure 7, the retrieval pattern across layers reveals the importance of RAPTOR’s\\nmulti-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR\\nusing the DPR retriever for the NarrativeQA dataset come from the first and second layers of the\\ntree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers,\\nalbeit with varying percentages.\\nTable 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers\\nDataset\\nDPR\\nSBERT\\nBM25\\nNarrativeQA\\n57.36%\\n36.78%\\n34.96%\\nQuality\\n32.28%\\n24.41%\\n32.36%\\nQasper\\n22.93%\\n18.49%\\n22.76%\\n22\\nPublished as a conference paper at ICLR 2024\\nTable 19: Percentage of nodes from different layers with DPR as the retriever\\nLayer\\nNarrativeQA\\nQuality\\nQasper\\n0\\n42.64%\\n67.71%\\n77.07%\\n1\\n45.00%\\n29.43%\\n21.88%\\n2\\n10.57%\\n2.85%\\n1.05%\\n3\\n1.78%\\n-\\n-\\n4\\n0.003%\\n-\\n-\\nTable 20: Percentage of nodes from different layers with SBERT as the retriever\\nLayer\\nNarrativeQA\\nQuality\\nQasper\\n0\\n63.22%\\n75.59%\\n81.51%\\n1\\n31.51%\\n22.78%\\n17.84%\\n2\\n4.85%\\n1.63%\\n0.65%\\n3\\n0.42%\\n-\\n-\\nTable 21: Percentage of nodes from different layers with BM25 as the retriever\\nLayer\\nNarrativeQA\\nQuality\\nQasper\\n0\\n65.04%\\n67.64%\\n77.24%\\n1\\n28.79%\\n28.85%\\n21.57%\\n2\\n5.36%\\n3.51%\\n1.19%\\n3\\n0.81%\\n-\\n-\\n23\\n' url='https://arxiv.org/abs/2401.18059'\n",
      "title='Lost in the Middle: How Language Models Use Long Contexts' text='Lost in the Middle: How Language Models Use Long Contexts\\nNelson F. Liu1∗\\nKevin Lin2\\nJohn Hewitt1\\nAshwin Paranjape3\\nMichele Bevilacqua3\\nFabio Petroni3\\nPercy Liang1\\n1Stanford University\\n2University of California, Berkeley\\n3Samaya AI\\nnfliu@cs.stanford.edu\\nAbstract\\nWhile recent language models have the abil-\\nity to take long contexts as input, relatively\\nlittle is known about how well they use\\nlonger context. We analyze the performance\\nof language models on two tasks that require\\nidentifying relevant information in their in-\\nput contexts: multi-document question an-\\nswering and key-value retrieval. We find that\\nperformance can degrade significantly when\\nchanging the position of relevant informa-\\ntion, indicating that current language models\\ndo not robustly make use of information in\\nlong input contexts. In particular, we observe\\nthat performance is often highest when rele-\\nvant information occurs at the beginning or\\nend of the input context, and significantly\\ndegrades when models must access relevant\\ninformation in the middle of long contexts,\\neven for explicitly long-context models. Our\\nanalysis provides a better understanding of\\nhow language models use their input context\\nand provides new evaluation protocols for\\nfuture long-context language models.\\n1\\nIntroduction\\nLanguage models have become an important and\\nflexible building block in a variety of user-facing\\nlanguage technologies, including conversational\\ninterfaces, search and summarization, and collabo-\\nrative writing (Shuster et al., 2022; Thoppilan et al.,\\n2022; Lee et al., 2022, inter alia). These models\\nperform downstream tasks primarily via prompting:\\nall relevant task specification and data to process is\\nformatted as a textual input context, and the model\\nreturns a generated text completion. These input\\ncontexts can contain thousands of tokens, espe-\\ncially when language models are used to process\\nlong documents (e.g., legal or scientific documents,\\nconversation histories, etc.) or when language mod-\\nels are augmented with external information (e.g.,\\n*Work partially completed as an intern at Samaya AI.\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n55\\n60\\n65\\n70\\n75\\nAccuracy\\n20 T\\notal Retrieved Documents (~4K tokens)\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-0613 (closed-book)\\nFigure 1: Changing the location of relevant information\\n(in this case, the position of the passage that answers an\\ninput question) within the language model’s input con-\\ntext results in a U-shaped performance curve—models\\nare better at using relevant information that occurs at the\\nvery beginning (primacy bias) or end of its input context\\n(recency bias), and performance degrades significantly\\nwhen models must access and use information located\\nin the middle of its input context.\\nrelevant documents from a search engine, database\\nquery results, etc; Petroni et al., 2020; Ram et al.,\\n2023; Shi et al., 2023; Mallen et al., 2023; Schick\\net al., 2023, inter alia).\\nHandling these use-cases requires language mod-\\nels to successfully operate over long sequences. Ex-\\nisting language models are generally implemented\\nwith Transformers (Vaswani et al., 2017), which re-\\nquire memory and compute that increases quadrat-\\nically in sequence length.\\nAs a result, Trans-\\nformer language models were often trained with\\nrelatively small context windows (between 512-\\n2048 tokens). Recent improvements in hardware\\n(e.g., faster GPUs with more memory) and algo-\\nrithms (Dai et al., 2019; Dao et al., 2022; Poli et al.,\\narXiv:2307.03172v3  [cs.CL]  20 Nov 2023\\n2023; Rubin and Berant, 2023, inter alia) have\\nresulted in language models with larger context\\nwindows (e.g., 4096, 32K, and even 100K tokens),\\nbut it remains unclear how these extended-context\\nlanguage models make use of their input contexts\\nwhen performing downstream tasks.\\nWe empirically investigate this question via\\ncontrolled experiments with a variety of state-of-\\nthe-art open (MPT-30B-Instruct, LongChat-13B\\n(16K)) and closed (OpenAI’s GPT-3.5-Turbo and\\nAnthropic’s Claude-1.3) language models in set-\\ntings that require accessing and using information\\nwithin an input context. In particular, our experi-\\nments make controlled changes to the input context\\nsize and the position of the relevant information\\nwithin the input context and study their effects on\\nlanguage model performance. If language models\\ncan robustly use information within long input con-\\ntexts, then their performance should be minimally\\naffected by the position of the relevant information\\nin the input context.\\nWe first experiment with multi-document ques-\\ntion answering, which requires models to reason\\nover provided documents to find relevant informa-\\ntion and use it to answer a given question; this task\\nmimics the retrieval-augmented generation setup\\nunderlying many commercial generative search and\\nquestion answering applications (e.g., Bing Chat).\\nIn this setting, we control (i) the input context\\nlength by changing the number of documents in\\nthe input context (akin to retrieving more or less\\ndocuments in retrieval-augmented generation), and\\n(ii) control the position of the relevant information\\nwithin the input context by changing the order of\\nthe documents to place the relevant document at\\nthe beginning, middle or end of the context.\\nWe find that changing the position of relevant\\ninformation in the input context can substantially\\naffect model performance, indicating that current\\nlanguage models do not robustly access and use\\ninformation in long input contexts. Furthermore,\\nwe observe a distinctive U-shaped performance\\ncurve (Figure 1); language model performance is\\nhighest when relevant information occurs at the\\nvery beginning (primacy bias) or end of its in-\\nput context (recency bias), and performance sig-\\nnificantly degrades when models must access and\\nuse information in the middle of their input con-\\ntext (§2.3).\\nFor example, when relevant infor-\\nmation is placed in the middle of its input con-\\ntext, GPT-3.5-Turbo’s performance on the multi-\\ndocument question task is lower than its perfor-\\nmance when predicting without any documents (i.e.,\\nthe closed-book setting; 56.1%). Furthermore, we\\nfind that models often have identical performance\\nto their extended-context counterparts, indicating\\nthat extended-context models are not necessarily\\nbetter at using their input context (§2.3).\\nGiven that language models struggle to retrieve\\nand use relevant information in the multi-document\\nquestion answering task, to what extent can lan-\\nguage models even retrieve from their input con-\\ntexts? We study this question with a synthetic key-\\nvalue retrieval task, which is designed to be a mini-\\nmal testbed for the basic ability to retrieve matching\\ntokens from the input context. In this task, models\\nare given a collection of JSON-formatted key-value\\npairs and must return the value associated with a\\nspecific key. Similar to the multi-document QA\\ntask, the key-value retrieval task admits controlled\\nchanges to the input context length (adding more\\nkey-value pairs) and the position of relevant in-\\nformation. Although some models perform the\\nsynthetic key-value retrieval task perfectly, other\\nmodels struggle to simply retrieve matching tokens\\nthat occur in the middle of their input context and\\ncontinue to exhibit a U-shaped performance curve.\\nTo better understand why language models strug-\\ngle to robustly access and use information in their\\ninput contexts, we study the role of model archi-\\ntecture (decoder-only vs. encoder-decoder), query-\\naware contextualization, and instruction fine-tuning\\n(§4). We find that:\\n• Encoder-decoder models are relatively robust\\nto changes in the position of relevant informa-\\ntion within their input context, but only when\\nevaluated on sequences within its training-\\ntime sequence length. When evaluated on\\nsequences longer than those seen during train-\\ning, we observe a U-shaped performance\\ncurve (§4.1).\\n• Query-aware contextualization (placing the\\nquery before and after the documents or key-\\nvalue pairs) enables near-perfect performance\\non the synthetic key-value task, but minimally\\nchanges trends in multi-document QA (§4.2).\\n• Even base language models (i.e., without in-\\nstruction fine-tuning) show a U-shaped per-\\nformance curve as we vary the position of\\nrelevant information in the input context.\\nOur results indicate that prompting language\\nmodels with longer input contexts is a trade-off—\\nproviding the language model with more informa-\\ntion may help it perform the downstream task, but\\nit also increases the amount of content that the\\nmodel must reason over, potentially decreasing ac-\\ncuracy. To better understand this trade-off in prac-\\ntice, we perform a case study with retriever-reader\\nmodels on open-domain question answering (§5).\\nIn contrast to our controlled multi-document QA\\ntask, where the context always contains exactly\\none document that answers the question, none or\\nmany of the top k documents may contain the an-\\nswer in the open-domain QA setting. When re-\\ntrieving from Wikipedia to answer queries from\\nNaturalQuestions-Open, we find that model perfor-\\nmance saturates long before retriever recall satu-\\nrates, indicating that current models fail to effec-\\ntively use additional retrieved documents—using\\n50 documents instead of 20 retrieved documents\\nonly marginally improves performance (∼1.5% for\\nGPT-3.5-Turbo and ∼1% for claude-1.3).\\nOur analysis provides a better understanding of\\nhow language models use their input context and\\nintroduces new evaluation protocols for future long-\\ncontext models; to claim that a language model can\\nrobustly use information within long input con-\\ntexts, it is necessary to show that its performance\\nis minimally affected by the position of the rele-\\nvant information in the input context (e.g., minimal\\ndifference in best- and worst-case performance).\\nTo facilitate further work on understanding and\\nimproving how language models use their input\\ncontext, we release our code and evaluation data.1\\n2\\nMulti-Document Question Answering\\nOur goal is to better understand how language mod-\\nels use their input context. To this end, we analyze\\nmodel performance on multi-document question\\nanswering, which requires models to find relevant\\ninformation within an input context and use it to\\nanswer the question. In particular, we make con-\\ntrolled changes to the length of the input context\\nand the position of the relevant information and\\nmeasure changes in task performance.\\n2.1\\nExperimental Setup\\nIn the multi-document question answering task, the\\nmodel inputs are (i) a question to answer and (ii) k\\ndocuments (e.g., passages from Wikipedia), where\\nexactly one of the documents contains the answer\\n1nelsonliu.me/papers/lost-in-the-middle\\nto the question and k −1 “distractor” documents\\ndo not. This task requires the model to access the\\ndocument that contains the answer within its input\\ncontext and use it to answer the question. Figure 2\\npresents an example.\\nWe\\ninstantiate\\nthis\\ntask\\nwith\\ndata\\nfrom\\nNaturalQuestions-Open\\n(Lee\\net\\nal.,\\n2019;\\nKwiatkowski et al., 2019),\\nwhich contains\\nhistorical queries issued to the Google search\\nengine, coupled with human-annotated answers\\nextracted from Wikipedia. In particular, we take\\nthe 2655 queries where the annotated long answer\\nis a paragraph (as opposed to a list or a table). We\\nuse passages (chunks of at most 100 tokens) from\\nWikipedia as documents within our input contexts.\\nFor each of the queries, we need a document\\nthat contains the answer and k −1 distractor\\ndocuments that do not contain the answer.\\nTo\\nobtain a document that answers the question, we\\nuse the Wikipedia paragraph that contains the\\nanswer from the NaturalQuestions annotations.\\nTo collect k −1 distractor documents that do not\\ncontain the answer, we use a retrieval system (Con-\\ntriever, fine-tuned on MS-MARCO; Izacard et al.,\\n2021) to retrieve the k −1 Wikipedia chunks that\\nare most relevant to the query and do not contain\\nany of the NaturalQuestions-annotated answers.2,3\\nIn the input context, the distractor documents are\\npresented in order of decreasing relevance.4\\nTo modulate the position of relevant information\\nwithin the input context, we adjust the order of the\\ndocuments to change the position of the document\\nthat contains the answer (Figure 3). To modulate\\nthe input context length in this task, we increase or\\ndecrease the number of retrieved documents that\\ndo not contain the answer (Figure 4).\\nFollowing Kandpal et al. (2022) and Mallen et al.\\n(2023), we use accuracy as our primary evaluation\\nmetric, judging whether any of the correct answers\\n(as taken from the NaturalQuestions annotations)\\nappear in the predicted output.\\n2Ambiguity in NaturalQuestions-Open means that a small\\nnumber of distractor passages may contain a reasonable an-\\nswer. We additionally run experiments on subset of unam-\\nbiguous questions, finding similar results and conclusions; see\\nAppendix A.\\n3We also explored using random documents as distractors,\\nsee Appendix B for more details.\\n4Since there might be a prior over “search results” appear-\\ning in ranked order, we explored randomly ordering the k −1\\ndistractor documents and mentioning that the documents are\\nrandomly ordered in the task description, but found the same\\ntrends. See Appendix C for more details.\\nWrite a high-quality answer for the given question using only the provided search \\nresults (some of which might be irrelevant).\\nDocument [1](Title: Asian Americans in science and technology) Prize in physics for \\ndiscovery of the subatomic particle J/ψ. Subrahmanyan Chandrasekhar shared...\\nDocument [2](Title: List of Nobel laureates in Physics) The first Nobel Prize in \\nPhysics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who received...\\nDocument [3](Title: Scientist) and pursued through a unique method, was essentially \\nin place. Ramón y Cajal won the Nobel Prize in 1906 for his remarkable...\\nQuestion: who got the first nobel prize in physics\\nAnswer:\\nInput Context\\nWilhelm Conrad Röntgen\\nDesired Answer\\nFigure 2: Example of the multi-document question answering task, with an input context and the desired model\\nanswer. The document containing the answer is bolded within the input context here for clarity.\\nWrite a high-quality answer for the given question \\nusing only the provided search results (some of \\nwhich might be irrelevant).\\nDocument [1](Title: List of Nobel laureates in \\nPhysics) ...\\nDocument [2](Title: Asian Americans in science and \\ntechnology) ...\\nDocument [3](Title: Scientist) ...\\nQuestion: who got the first nobel prize in physics\\nAnswer:\\nInput Context\\nWilhelm Conrad Röntgen\\nDesired Answer\\nFigure 3: Modulating the position of relevant informa-\\ntion within the input context for the multi-document\\nquestion answering example presented in Figure 2. Re-\\nordering the documents in the input context does not\\naffect the desired output.\\nOur experimental setup is similar to the needle-\\nin-a-haystack experiments of Ivgi et al. (2023), who\\ncompare question answering performance when the\\nrelevant paragraph is placed (i) at the beginning of\\nthe input or (ii) a random position within the in-\\nput. They find that encoder-decoder models have\\nsignificantly higher performance when relevant in-\\nformation is placed at the start of the input context.\\nIn contrast, we study finer-grained changes in the\\nposition of relevant information.\\n2.2\\nModels\\nWe analyze several state-of-the-art open and closed\\nlanguage models. We use greedy decoding when\\ngenerating outputs and leave exploration of other\\ndecoding methods to future work. We use a stan-\\ndard set of prompts for each model (Figure 2).\\nWrite a high-quality answer for the given question \\nusing only the provided search results (some of \\nwhich might be irrelevant).\\nDocument [1](Title: Asian Americans in science and \\ntechnology) ...\\nDocument [2](Title: List of Nobel laureates in \\nPhysics) ...\\nDocument [3](Title: Scientist) ...\\nDocument [4](Title: Norwegian Americans) ...\\nDocument [5](Title: Maria Goeppert Mayer) ...\\nQuestion: who got the first nobel prize in physics\\nAnswer:\\nInput Context\\nInput Context\\nWilhelm Conrad Röntgen\\nDesired Answer\\nFigure 4: Modulating the input context length of the\\nmulti-document question answering example presented\\nin Figure 2. Adding documents that do not contain the\\nanswer increases the length of the input context, but\\ndoes not affect the desired output.\\nOpen models.\\nWe experiment with MPT-30B-\\nInstruct, which has a maximum context length of\\n8192 tokens. The model was initially pre-trained\\non 1 trillion tokens using 2048-token sequences,\\nfollowed by an additional sequence length adapta-\\ntion pre-training phase on 50 billion tokens using\\n8192-token sequences. MPT-30B-Instruct uses AL-\\niBi (Press et al., 2022) to represent positional infor-\\nmation. We also evaluate LongChat-13B (16K) (Li\\net al., 2023), which extends the LLaMA-13B (Tou-\\nvron et al., 2023a) context window from 2048 to\\n16384 tokens by using condensed rotary positional\\nembeddings before fine-tuning with 16384-token\\nsequences.\\nClosed models.\\nWe use the OpenAI API to ex-\\nperiment with GPT-3.5-Turbo and GPT-3.5-Turbo\\n1st\\n5th\\n10th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\n75\\nAccuracy\\n10 T\\notal Retrieved Documents (~2K tokens)\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\n75\\nAccuracy\\n20 T\\notal Retrieved Documents (~4K tokens)\\n1st\\n5th\\n10th\\n15th\\n20th\\n25th\\n30th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\n75\\nAccuracy\\n30 T\\notal Retrieved Documents (~6K tokens)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 5: The effect of changing the position of relevant information (document containing the answer) on multi-\\ndocument question answering performance. Lower positions are closer to the start of the input context. Performance\\nis highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models\\nmust reason over information in the middle of their input context.\\n(16K).5 GPT-3.5-Turbo has a maximum context\\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\\nversion with an extended maximum context length\\nof 16K tokens. We evaluate Claude-1.3 and Claude-\\n1.3 (100K) with the Anthropic API; Claude-1.3\\nhas a maximum context length of 8K tokens, and\\nClaude-1.3 (100K) has an extended context length\\nof 100K tokens. 6\\n2.3\\nResults and Discussion\\nWe experiment with input contexts containing 10,\\n20, and 30 total documents. Figure 5 presents multi-\\ndocument question answering performance when\\nvarying the position of relevant information within\\nthe input context. To contextualize model perfor-\\nmance, we also evaluate on the closed-book and\\noracle settings (Table 1). In the closed-book setting,\\nmodels are not given any documents in their input\\ncontext, and must rely on their parametric memory\\nto generate the correct answer. On the other hand,\\nin the oracle setting, language models are given the\\nsingle document that contains the answer and must\\nuse it to answer the question.\\nModel performance is highest when relevant in-\\nformation occurs at the beginning or end of its\\ninput context.\\nAs illustrated in Figure 5, chang-\\ning the position of relevant information in the in-\\nput context leads to substantial decreases in model\\nperformance. In particular, we see a distinctive U-\\n5We use the 0613 OpenAI model versions.\\n6We also evaluate GPT-4 (8K) on a subset of multi-\\ndocument QA experiments, finding similar results and trends\\nas other models (though GPT-4 has higher absolute perfor-\\nmance). Evaluating GPT-4 on the full multi-document QA\\nand key-value retrieval experiments would cost upwards of\\n$6000. See Appendix D for GPT-4 results and discussion.\\nModel\\nClosed-Book\\nOracle\\nLongChat-13B (16K)\\n35.0%\\n83.4%\\nMPT-30B-Instruct\\n31.5%\\n81.9%\\nGPT-3.5-Turbo\\n56.1%\\n88.3%\\nGPT-3.5-Turbo (16K)\\n56.0%\\n88.6%\\nClaude-1.3\\n48.3%\\n76.1%\\nClaude-1.3 (100K)\\n48.2%\\n76.4%\\nTable 1: Closed-book and oracle accuracy of language\\nmodels on the multi-document question answering task.\\nshaped performance curve—models are often much\\nbetter at using relevant information that occurs at\\nthe very beginning (primacy bias) and very end of\\ncontexts (recency bias), and suffer degraded perfor-\\nmance when forced to use information within the\\nmiddle of its input context. For example, GPT-3.5-\\nTurbo’s multi-document QA performance can drop\\nby more than 20%—in the worst case, performance\\nin 20- and 30-document settings is lower than per-\\nformance without any input documents (i.e., closed-\\nbook performance; 56.1%). These results indicate\\nthat current models cannot effectively reason over\\ntheir entire context window when prompted for\\ndownstream tasks.\\nExtended-context models are not necessarily bet-\\nter at using input context.\\nWhen the input con-\\ntext fits in the context window of both a model\\nand its extended-context counterpart, we see that\\nperformance between them is nearly identical. For\\nexample, the 10- and 20-document settings both\\nfit in the context window of GPT-3.5-Turbo and\\nGPT-3.5-Turbo (16K), and we observe that their\\nperformance as a function of position of relative\\ninformation is nearly superimposed (solid purple\\nand dashed brown series in Figure 5). These results\\nExtract the value corresponding to the specified key in the JSON object below.\\nJSON data:\\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\",\\n \"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\\n \"9f4a92b9-5f69-4725-ba1e-403f08dea695\": \"703a7ce5-f17f-4e6d-b895-5836ba5ec71c\",\\n \"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-b9823991ebeb\",\\n \"f4eb1c53-af0a-4dc4-a3a5-c2d50851a178\": \"d733b0d2-6af3-44e1-8592-e5637fdb76fb\"}\\nKey: \"9f4a92b9-5f69-4725-ba1e-403f08dea695\"\\nCorresponding value:\\nInput Context\\n703a7ce5-f17f-4e6d-b895-5836ba5ec71c\\nDesired Output\\nFigure 6: Example of the key-value retrieval task, with an input context and the desired model output. Given a key,\\nthe goal is to return the associated value. All keys and values are 128-bit UUIDs. The relevant key-value pair for\\nanswering the query is bolded here within the input context for clarity.\\nindicate that extended-context models are not nec-\\nessarily better than their non-extended counterparts\\nat using their input context.\\n3\\nHow Well Can Language Models\\nRetrieve From Input Contexts?\\nGiven that language models struggle to retrieve\\nand use information from the middle of their input\\ncontexts in the multi-document question answering\\ntask, to what extent can they simply retrieve from\\ninput contexts? We study this question with a syn-\\nthetic key-value retrieval task, which is designed to\\nprovide a minimal testbed for the basic ability to\\nretrieve matching tokens from an input context.\\n3.1\\nExperimental Setup\\nIn our synthetic key-value retrieval task, the inputs\\nare (i) a string-serialized JSON object with k key-\\nvalue pairs, where each of the keys and values are\\nunique, randomly-generated UUIDs and (ii) a key\\nwithin the aforementioned JSON object. The goal\\nis to return the value associated with the specified\\nkey. Thus, each JSON object contains one relevant\\nkey-value pair (where the value is to be returned),\\nand k −1 irrelevant “distractor” key-value pairs.\\nFigure 6 provides an example input context and its\\ncorresponding desired output. We again measure\\naccuracy by evaluating whether the correct value\\nappears in the predicted output.\\nOur synthetic key-value retrieval task shares sim-\\nilar goals with the Little Retrieval Test of Papail-\\niopoulos et al. (2023) and the fine-grained line re-\\ntrieval task of Li et al. (2023), but we explicitly\\nseek to distill and simplify the task by removing as\\nmuch natural language semantics as possible (using\\nrandom UUIDs instead), since language features\\nmay present potential confounders. For example,\\nTransformer language models may have varying\\nsensitivity to different linguistic features in their\\ninput (O’Connor and Andreas, 2021).\\nTo modulate the position of relevant information\\nwithin the input context, we change the position\\nof the key to retrieve within the serialized JSON\\nobject. To modulate the input context length, we\\nchange the number of input JSON key-value pairs\\nk by adding or removing random keys, changing\\nthe number of distractor key-value pairs.\\n3.2\\nResults and Discussion\\nWe experiment with input contexts containing\\n75, 140, and 300 key-value pairs (500 examples\\neach). We use the same set of models as the multi-\\ndocument question answering experiments, see\\n§2.2 for more details.\\nFigure 7 presents key-value retrieval perfor-\\nmance.\\nClaude-1.3 and Claude-1.3 (100K) do\\nnearly perfectly on all evaluated input context\\nlengths, but other models struggle, especially\\nwhen contexts have 140 or 300 key-value pairs—\\nalthough the synthetic key-value retrieval task only\\nrequires identifying exact match within the input\\ncontext, not all models achieve high performance.\\nSimilar to our multi-document QA results, GPT-\\n3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\\nInstruct have the lowest performance when they\\nmust access key-value pairs in the middle of their\\ninput context. LongChat-13B (16K) exhibits a dif-\\nferent trend in the 140 key-value setting; we quali-\\ntatively observe that when relevant information is\\n1st\\n25th\\n50th\\n75th\\nPosition of Key to Retrieve\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy\\n75 Key-Value Pairs (~4K tokens)\\n1st\\n35th\\n70th\\n105th\\n140th\\nPosition of Key to Retrieve\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy\\n140 Key-Value Pairs (~8K tokens)\\n1st\\n50th\\n100th 150th 200th 250th 300th\\nPosition of Key to Retrieve\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy\\n300 Key-Value Pairs (~16K tokens)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 7: The effect of changing the input context length and the position of relevant information on key-value\\nretrieval performance. Lower positions are closer to the start of the input context. Although some models show\\nperfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is\\noften highest when relevant information is occurs at the very start or end of the context, and rapidly degrades when\\nmodels must retrieve from the middle of the input context.\\nplaced at the start of the input context, LongChat-\\n13B (16K) tends to generate code to retrieve the\\nkey, rather than outputting the value directly.\\n4\\nWhy Are Language Models Not Robust\\nto Changes in the Position of Relevant\\nInformation?\\nOur multi-document question answering and key-\\nvalue retrieval results show that language models\\nstruggle to robustly access and use information in\\nlong input contexts, since performance degrades\\nsignificantly when changing the position of rele-\\nvant information. To better understand why, we per-\\nform some preliminary investigations into the role\\nof model architecture (decoder-only vs. encoder-\\ndecoder), query-aware contextualization, and in-\\nstruction fine-tuning.\\n4.1\\nEffect of Model Architecture\\nThe open models we evaluated are all decoder-only\\nmodels—at each timestep, they may only attend\\nto prior tokens. To better understand the poten-\\ntial effects of model architecture on how language\\nmodel use context, we compare decoder-only and\\nencoder-decoder language models.\\nWe experiment with Flan-T5-XXL (Raffel et al.,\\n2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,\\n2023). Flan-T5-XXL is trained with a sequences\\nof 512 tokens (encoder and decoder). Flan-UL2 is\\ninitially trained with sequences of 512 tokens (en-\\ncoder and decoder), but is then pre-trained for an\\nextra 100K steps with 1024 tokens (encoder and de-\\ncoder) before instruction fine-tuning on sequences\\nwith 2048 tokens in the encoder and 512 tokens\\nin the decoder. However, since these models use\\nrelative positional embeddings, they can (in prin-\\nciple) extrapolate beyond these maximum context\\nlengths; Shaham et al. (2023) find that both mod-\\nels can perform well with sequences of up to 8K\\ntokens.\\nFigure 8 compares the performance of decoder-\\nonly and encoder-decoder models. When Flan-UL2\\nis evaluated on sequences within its 2048-token\\ntraining-time context window (Figure 8; left sub-\\nplot), its performance is relatively robust to changes\\nin the position of relevant information within the\\ninput context (1.9% absolute difference between\\nbest- and worst-case performance). When evalu-\\nated on settings with sequences longer than 2048\\ntokens (Figure 8; center and right), Flan-UL2 per-\\nformance begins to degrade when relevant informa-\\ntion is placed in the middle. Flan-T5-XXL shows\\na similar trend, where longer input contexts result\\nin a greater performance degradation when placing\\nrelevant information in the middle of the input con-\\ntext. We hypothesize that encoder-decoder models\\nmay make better use of their context windows be-\\ncause their bidirectional encoder allows processing\\neach document in the context of future documents,\\npotentially improving relative importance estima-\\ntion between documents.\\n4.2\\nEffect of Query-Aware Contextualization\\nOur multi-document QA and key-value retrieval\\nexperiments place the query (i.e., question to an-\\nswer or key to retrieve) after the data to process\\n(i.e., the documents or the key-value pairs). As a\\nresult, decoder-only models cannot attend to query\\ntokens when contextualizing documents or key-\\nvalue pairs, since the query only appears at the end\\n1st\\n5th\\n10th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\nAccuracy\\n10 T\\notal Retrieved Documents (~2K tokens)\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\nAccuracy\\n20 T\\notal Retrieved Documents (~4K tokens)\\n1st\\n5th\\n10th\\n15th\\n20th\\n25th\\n30th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70\\nAccuracy\\n30 T\\notal Retrieved Documents (~6K tokens)\\nmpt-30b-instruct\\nlongchat-13b-16k\\nflan-t5-xxl\\nflan-ul2\\nFigure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter\\nthan their encoder’s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively\\nrobust to changes in the position of relevant information within their input context (left subplot). In contrast, when\\nthese models are evaluated on sequences longer than those seen during training (center and right subplots), we\\nobserve a U-shaped performance curve—performance is higher when relevant information occurs at the beginning\\nor end of the input context, as opposed to the middle of the input context.\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n50\\n60\\n70\\n80\\nAccuracy\\n20 T\\notal Retrieved Documents \\n(~4K tokens, query-aware contextualization)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 9: Query-aware contextualization (placing the\\nquery before and after the documents) does not sub-\\nstantially improve robustness of language models to\\nchanging the position of relevant information in multi-\\ndocument QA; performance slightly increases when\\nrelevant information occurs at the very beginning, but\\notherwise slightly decreases.\\nof the prompt and decoder-only models can only\\nattend to prior tokens at each timestep. In contrast,\\nencoder-decoder models (which seem more robust\\nto changes in the position of relevant information;\\n§4.1) use a bidirectional encoder to contextualize\\ninput contexts—can we use this observation to im-\\nprove decoder-only models by placing the query be-\\nfore and after the data, enabling query-aware con-\\ntextualization of documents (or key-value pairs)?\\nWe find that query-aware contextualization dra-\\nmatically improves performance on the key-value\\nretrieval task—all models achieve near-perfect per-\\nformance on the 75, 140, and 300 key-value pair\\nsettings. For example, GPT-3.5-Turbo (16K) with\\nquery-aware contextualization achieves perfect per-\\nformance when evaluated with 300 key-value pairs.\\nIn contrast, without query-aware contextualiza-\\ntion, the worst-case performance is 45.6% (Fig-\\nure 7).\\nDespite the significant impact on key-\\nvalue retrieval performance, query-aware contextu-\\nalization minimally affects performance trends in\\nthe multi-document question answering task (Fig-\\nure 9); it slightly improves performance when the\\nrelevant information is located at the very begin-\\nning of the input context, but slightly decreases\\nperformance in other settings.\\n4.3\\nEffect of Instruction Fine-Tuning\\nThe models we evaluated are all instruction fine-\\ntuned—after their initial pre-training, they undergo\\nsupervised fine-tuning on a dataset of instructions\\nand responses. The task specification and/or in-\\nstruction is commonly placed at the beginning of\\nthe input context in supervised instruction fine-\\ntuning data, which might lead instruction fine-\\ntuned language models to place more weight on\\nthe start of the input context. To better understand\\nthe potential effects of instruction fine-tuning on\\nhow language models use long input contexts, we\\ncompare the multi-document question answering\\nperformance of MPT-30B-Instruct against its base\\nmodel (i.e., before instruction fine-tuning) MPT-\\n30B. We use the same experimental setup as §2.\\nFigure 10 compares the multi-document QA\\nperformance of MPT-30B and MPT-30B-Instruct\\nas a function of the position of the relevant in-\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n44\\n46\\n48\\n50\\n52\\n54\\n56\\nAccuracy\\n20 T\\notal Retrieved Documents (~4K tokens)\\nmpt-30b\\nmpt-30b-instruct\\nFigure 10: Multi-document QA performance of MPT-\\n30B-Instruct compared against its base model (i.e., be-\\nfore instruction fine-tuning) MPT-30B. Both models\\nhave a U-shaped performance curve, where performance\\nis much higher when relevant information occurs at the\\nstart or end of the input context, indicating that the\\ninstruction fine-tuning process itself is not necessarily\\nresponsible for these performance trends.\\nformation in the input context. Surprisingly, we\\nsee that both MPT-30B and MPT-30B-Instruct ex-\\nhibit a U-shaped performance curve, where perfor-\\nmance is highest when relevant information occurs\\nat the very beginning or very end of the context.\\nAlthough the absolute performance of MPT-30B-\\nInstruct is uniformly higher than that of MPT-30B,\\ntheir overall performance trends are similar. We\\nalso observe that instruction fine-tuning slightly re-\\nduces the worst-case performance disparity from\\nnearly 10% between the base model best- and\\nworst-case performance to around 4%.\\nThese observations complement prior work,\\nwhich found that non-instruction fine-tuned lan-\\nguage models are biased towards recent tokens (i.e.,\\nthe end of the input context; Khandelwal et al.,\\n2018; Press et al., 2021). This recency bias has\\nbeen observed in past work when evaluating mod-\\nels on next-word prediction of contiguous text, a\\nsetting where language models minimally benefit\\nfrom long-range information (Sun et al., 2021). In\\ncontrast, our results show that language models\\nare capable of using longer-range information (i.e.,\\nthe beginning of the input context) when prompted\\nwith instruction-formatted data. We hypothesize\\nthat non-instruction fine-tuned language models\\nlearn to use these long contexts from similarly-\\nformatted data that may occur in Internet text seen\\nduring pre-training, e.g., StackOverflow questions\\nand answers.\\nTo better understand the effect of additional fine-\\ntuning and model scale, we also experimented\\nwith Llama-2 models of varying sizes (7B, 13B,\\nand 70B) with and without additional supervised\\nfine-tuning and reinforcement learning from hu-\\nman feedback (Appendix E). We find that the U-\\nshaped performance curve only appears in suffi-\\nciently large language models (with or without ad-\\nditional fine-tuning)—the 7B Llama-2 models are\\nsolely recency biased, while the 13B and 70B mod-\\nels exhibit a U-shaped performance curve. In addi-\\ntion, we see that the Llama-2 supervised fine-tuning\\nand reinforcement learning from human feedback\\nprocedure slightly mitigates the positional bias in\\nsmaller models (13B, akin to trends shown when\\ncomparing MPT-30B and MPT-30B-Instruct), but\\nminimally affects trends on larger models (70B).\\n5\\nIs More Context Is Always Better?\\nA Case Study With Open-Domain QA\\nOur results indicate that prompting language mod-\\nels with longer input contexts is a trade-off—\\nproviding the language model with more informa-\\ntion may help it perform the downstream task, but\\nit also increases the amount of content that the\\nmodel must reason over, potentially decreasing\\naccuracy. Even if a language model can take in\\n16K tokens, is it actually beneficial to provide 16K\\ntokens of context? The answer to this question\\nis ultimately downstream task-specific since it de-\\npends on the marginal value of the added context\\nand the model’s ability to effectively use long input\\ncontexts, but we perform a case study with open-\\ndomain question answering on NaturalQuestions-\\nOpen to better understand this trade-off in existing\\nlanguage models.\\nWe use language models in a standard retriever-\\nreader setup. A retrieval system (Contriever, fine-\\ntuned on MS-MARCO) takes an input query from\\nNaturalQuestions-Open and returns the k docu-\\nments from Wikipedia with the highest relevance\\nscore. To condition language models on these re-\\ntrieved documents, we simply include them in the\\nprompt. We evaluate retriever recall and reader\\naccuracy (whether any of the annotated answers\\nappear in the predicted output) as a function of the\\nnumber of retrieved documents k. We use a subset\\nof NaturalQuestions-Open where the long answer\\nis a paragraph (as opposed to a table or a list).\\nFigure 11 presents retriever recall and open-\\n5\\n10\\n20\\n30\\n40\\n50\\nNumber of Retrieved Docs\\n50\\n60\\n70\\n80\\n90\\nMetric\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\ncontriever recall\\nFigure 11: Retriever recall and model performance as a\\nfunction of the number of retrieved documents. Model\\nperformance saturates long before retriever recall, indi-\\ncating that the models have difficulty making use of the\\nextra retrieved documents.\\ndomain QA results. We see that reader model\\nperformance saturates long before retriever per-\\nformance saturates, indicating that readers are not\\neffectively using the extra context. Using more\\nthan 20 retrieved documents only marginally im-\\nproves reader performance (∼1.5% for GPT-3.5-\\nTurbo and ∼1% for Claude-1.3), while significantly\\nincreasing the input context length (and thus la-\\ntency and cost). These results, coupled with the\\nobservation that models are often better at retriev-\\ning and using information at the start or end of\\nthe input contexts, suggest that effective rerank-\\ning of retrieved documents (pushing relevant infor-\\nmation closer to the start of the input context) or\\nranked list truncation (retrieving fewer documents\\nwhen appropriate; Arampatzis et al., 2009) may be\\npromising directions for improving how language-\\nmodel-based readers use retrieved context.\\n6\\nRelated Work\\n6.1\\nLong-Context Language Models\\nThere is much prior work in designing performant\\nlanguage models with cheaper scaling than Trans-\\nformers in the context length. Many lines of work\\npursue Transformer variants with attention modi-\\nfications like recurrence (Dai et al., 2019), factor-\\nizing attention into computationally less intensive\\napproximations (Beltagy et al., 2020; Zaheer et al.,\\n2020), or low-rank approximations (Wang et al.,\\n2020; Peng et al., 2021). Dao et al. (2022) in-\\nstead provide a faster exact attention by a carefully-\\ncrafted IO-aware CUDA kernel. Separately, there\\nare attempts to do away with attention entirely to\\nremove quadratic sequence length complexity, of-\\nten through convolution and/or linear RNNs, e.g.,\\nin RWKV (Peng, 2023), S4 (Gu et al., 2022), or\\nHyena (Poli et al., 2023). Many prior efforts evalu-\\nate perplexity on a diverse web corpus as a proxy\\nfor the ability to process long contexts; this work\\nshows that precise knowledge access on long con-\\ntexts may be an added challenge.\\n6.2\\nHow Do Language Models Use Context?\\nThe pioneering work of Khandelwal et al. (2018)\\nshowed that small LSTM language models make\\nincreasingly coarse use of longer-term context;\\nSankar et al. (2019) found similar results in di-\\nalogue models. In a similar vein, Daniluk et al.\\n(2017) find that attentive LSTM language mod-\\nels tend to mainly use recent history.\\nPetroni\\net al. (2020) were among the first to demonstrate\\nthe potential of combining context from an in-\\nformation retrieval system with a pretrained lan-\\nguage models for unsupervised question answering.\\nO’Connor and Andreas (2021) found that many\\ninformation-destroying operations had marginal ef-\\nfects on Transformer LMs’ predictions. Krishna\\net al. (2022) found that long-context neural gen-\\neration in modestly-sized Transformer language\\nmodels degenerates because models fail to prop-\\nerly condition on long context. Finally, studying\\nlong-context models, Sun et al. (2021) found that\\nlonger contexts improves prediction of only a few\\ntokens, an empirical finding consistent with the\\ntheory of Sharan et al. (2018), who showed that\\nsequence distributions with bounded mutual infor-\\nmation necessarily lead to marginal average predic-\\ntion benefits from increasingly long context. Qin\\net al. (2023) analyze how efficient Transformers\\nperform on a variety of long-context downstream\\nNLP tasks, finding that long-context transformers\\nare recency-biased and do not effectively use long-\\nrange context.\\n6.3\\nThe Serial-Position Effect\\nThe U-shaped curve we observe in this work has\\na connection in psychology known as the serial-\\nposition effect (Ebbinghaus, 1913; Murdock Jr,\\n1962), that states that in free-association recall\\nof elements from a list, humans tend to best re-\\nmember the first and last elements of the list. The\\nserial-position effect plays a role in understanding\\nhow humans develop short- and long-term mem-\\nory. Observing a serial-position-like effect in lan-\\nguage models is perhaps surprising, since the self-\\nattention mechanisms underlying Transformer lan-\\nguage models is technically equally capable of re-\\ntrieving any token from their contexts.\\n7\\nConclusion\\nWe empirically study how language models use\\nlong input contexts via a series of controlled ex-\\nperiments. We show that language model perfor-\\nmance degrades significantly when changing the\\nposition of relevant information, indicating that\\nmodels struggle to robustly access and use infor-\\nmation in long input contexts. In particular, per-\\nformance is often lowest when models must use\\ninformation in the middle of long input contexts.\\nWe conduct a preliminary investigation of the role\\nof (i) model architecture, (ii) query-aware contextu-\\nalization, and (iii) instruction fine-tuning to better\\nunderstand how they affect how language models\\nuse context. Finally, we conclude with a practi-\\ncal case study of open-domain question answering,\\nfinding that the performance of language model\\nreaders saturates far before retriever recall. Our\\nresults and analysis provide a better understanding\\nof how language models use their input context\\nand provides new evaluation protocols for future\\nlong-context models.\\nAcknowledgments\\nWe would like to thank Luke Zettlemoyer, who\\nserved as our TACL action editor, and the the\\nanonymous reviewers for their comments and feed-\\nback. We also thank Claudiu Leoveanu-Condrei,\\nMegan Leszczynski, Dmytro Okhonko, Maithra\\nRaghu, Eric Wallace and Sang Michael Xie for\\nfeedback and discussions that helped improve this\\nwork. Further, we are grateful to Sewon Min for\\nher help with the AmbigQA dataset. This work\\nwas supported by the Stanford Center for Research\\non Foundation Models (CRFM), by OpenAI via\\nan API credits grant to the Stanford CRFM, and\\nby Anthropic via the Claude academic access pro-\\ngram.\\nReferences\\nAvi Arampatzis, Jaap Kamps, and Stephen Robert-\\nson. 2009. Where to stop reading a ranked list?\\nthreshold optimization using truncated score dis-\\ntributions. In Proc. of SIGIR.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\\n2020. Longformer: The long-document trans-\\nformer. ArXiv:2004.05150.\\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\\nret Zoph, Yi Tay, William Fedus, Yunxuan Li,\\nXuezhi Wang, Mostafa Dehghani, Siddhartha\\nBrahma, Albert Webson, Shixiang Shane Gu,\\nZhuyun Dai, Mirac Suzgun, Xinyun Chen,\\nAakanksha Chowdhery, Alex Castro-Ros, Marie\\nPellat, Kevin Robinson, Dasha Valter, Sharan\\nNarang, Gaurav Mishra, Adams Yu, Vincent\\nZhao, Yanping Huang, Andrew Dai, Hongkun\\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\\nDevlin, Adam Roberts, Denny Zhou, Quoc V.\\nLe, and Jason Wei. 2022. Scaling instruction-\\nfinetuned language models. ArXiv:2210.11416.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\\n2019. Transformer-XL: Attentive language mod-\\nels beyond a fixed-length context. In Proc. of\\nACL.\\nMichał Daniluk, Tim Rocktäschel, Johannes Welbl,\\nand Sebastian Riedel. 2017. Frustratingly short\\nattention spans in neural language modeling. In\\nProc. of ICLR.\\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. FlashAttention: Fast\\nand memory-efficient exact attention with IO-\\nawareness. ArXiv:2205.14135.\\nHermann Ebbinghaus. 1913. Memory: A contribu-\\ntion to experimental psychology. H. A. Ruger &\\nC. E. Bussenius, Trans.\\nAlbert Gu, Karan Goel, and Christopher Ré. 2022.\\nEfficiently modeling long sequences with struc-\\ntured state spaces. In Proc. of ICLR.\\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\\nEfficient long-text understanding with short-text\\nmodels.\\nTransactions of the Association for\\nComputational Linguistics, 11:284–299.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\\nSebastian Riedel, Piotr Bojanowski, Armand\\nJoulin, and Edouard Grave. 2021. Unsupervised\\ndense information retrieval with contrastive\\nlearning. ArXiv:2112.09118.\\nGautier Izacard and Edouard Grave. 2021. Lever-\\naging passage retrieval with generative models\\nfor open domain question answering. In Proc.\\nof EACL.\\nNikhil Kandpal, Haikang Deng, Adam Roberts,\\nEric Wallace, and Colin Raffel. 2022. Large lan-\\nguage models struggle to learn long-tail knowl-\\nedge. ArXiv:2211.08411.\\nUrvashi Khandelwal, He He, Peng Qi, and Dan\\nJurafsky. 2018. Sharp nearby, fuzzy far away:\\nHow neural language models use context. In\\nProc. of ACL.\\nKalpesh Krishna, Yapei Chang, John Wieting, and\\nMohit Iyyer. 2022. RankGen: Improving text\\ngeneration with large ranking models. In Proc.\\nof EMNLP.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\\nRedfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\\nDevlin, Kenton Lee, Kristina Toutanova, Llion\\nJones, Matthew Kelcey, Ming-Wei Chang, An-\\ndrew M. Dai, Jakob Uszkoreit, Quoc Le, and\\nSlav Petrov. 2019. Natural Questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics, 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina\\nToutanova. 2019. Latent retrieval for weakly\\nsupervised open domain question answering. In\\nProc. of ACL.\\nMina Lee, Percy Liang, and Qian Yang. 2022.\\nCoAuthor: Designing a human-AI collaborative\\nwriting dataset for exploring language model ca-\\npabilities. In Proc. of CHI.\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng,\\nLianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\\nXuezhe Ma, , and Hao Zhang. 2023. How long\\ncan open-source LLMs truly promise on context\\nlength?\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\\nDas, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. When not to trust language models: In-\\nvestigating effectiveness of parametric and non-\\nparametric memories. In Proc. of ACL.\\nSewon Min, Julian Michael, Hannaneh Hajishirzi,\\nand Luke Zettlemoyer. 2020. AmbigQA: An-\\nswering ambiguous open-domain questions. In\\nProc. of EMNLP.\\nBennet B. Murdock Jr. 1962. The serial position\\neffect of free recall. Journal of experimental\\npsychology, 64(5):482.\\nJoe O’Connor and Jacob Andreas. 2021. What con-\\ntext features can Transformer language models\\nuse? In Proc. of ACL.\\nDimitris Papailiopoulos, Kangwook Lee, and Jy-\\nyong Sohn. 2023.\\nA little retrieval test for\\nlarge language models. https://github.com/\\nanadim/the-little-retrieval-test.\\nBo Peng. 2023. RWKV-LM. https://github.\\ncom/BlinkDL/RWKV-LM.\\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\\nSchwartz, Noah Smith, and Lingpeng Kong.\\n2021. Random feature attention. In Proc. of\\nICLR.\\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\\nTim Rocktäschel, Yuxiang Wu, Alexander H\\nMiller, and Sebastian Riedel. 2020. How context\\naffects language models’ factual predictions. In\\nProc. of AKBC.\\nMichael Poli, Stefano Massaroli, Eric Nguyen,\\nDaniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua\\nBengio, Stefano Ermon, and Christopher Ré.\\n2023. Hyena hierarchy: Towards larger con-\\nvolutional language models. In Proc. of ICML.\\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\\nShortformer: Better language modeling using\\nshorter inputs. In Proc. of ACL.\\nOfir Press, Noah A. Smith, and Mike Lewis. 2022.\\nTrain short, test long: Attention with linear bi-\\nases enables input length extrapolation. In Proc.\\nof ICLR.\\nGuanghui Qin,\\nYukun Feng,\\nand Benjamin\\nVan Durme. 2023. The NLP task effectiveness\\nof long-range transformers. In Proc. of EACL.\\nColin Raffel, Noam Shazeer, Adam Roberts,\\nKatherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex-\\nploring the limits of transfer learning with a uni-\\nfied text-to-text Transformer. Journal of Ma-\\nchine Learning Research, 21(140):1–67.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor\\nMuhlgay, Amnon Shashua, Kevin Leyton-\\nBrown,\\nand\\nYoav\\nShoham.\\n2023.\\nIn-\\ncontext retrieval-augmented language models.\\nArXiv:2302.00083.\\nOhad Rubin and Jonathan Berant. 2023. Long-\\nrange language modeling with self-retrieval.\\nArXiv:2306.13421.\\nChinnadhurai Sankar, Sandeep Subramanian, Chris\\nPal, Sarath Chandar, and Yoshua Bengio. 2019.\\nDo neural dialog systems use the conversation\\nhistory effectively? an empirical study. In Proc.\\nof ACL.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì,\\nRoberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom.\\n2023. Toolformer: Language models can teach\\nthemselves to use tools.\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-\\nrant, and Omer Levy. 2023. ZeroSCROLLS: A\\nzero-shot benchmark for long text understanding.\\nArXiv:2305.14196.\\nVatsal Sharan, Sham Kakade, Percy Liang, and\\nGregory Valiant. 2018. Prediction with a short\\nmemory. In Proc. of STOC.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen tau Yih. 2023.\\nREPLUG:\\nRetrieval-augmented black-box language mod-\\nels. ArXiv:2301.12652.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\\nEric Michael Smith, Stephen Roller, Megan\\nUng, Moya Chen, Kushal Arora, Joshua Lane,\\nMorteza Behrooz, William Ngan, Spencer Poff,\\nNaman Goyal, Arthur Szlam, Y-Lan Boureau,\\nMelanie Kambadur, and Jason Weston. 2022.\\nBlenderBot 3: a deployed conversational agent\\nthat continually learns to responsibly engage.\\nArXiv:2208.03188.\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\\nMicke, and Mohit Iyyer. 2021. Do long-range\\nlanguage models actually use long-range con-\\ntext? In Proc. of EMNLP.\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier\\nGarcia, Jason Wei, Xuezhi Wang, Hyung Won\\nChung,\\nSiamak Shakeri,\\nDara Bahri,\\nTal\\nSchuster, Huaixiu Steven Zheng, Denny Zhou,\\nNeil Houlsby,\\nand Donald Metzler. 2023.\\nUL2: Unifying language learning paradigms.\\nArXiv:2205.05131.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-\\nTze Cheng, Alicia Jin, Taylor Bos, Leslie\\nBaker, Yu Du, YaGuang Li, Hongrae Lee,\\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo\\nMenegali, Yanping Huang, Maxim Krikun,\\nDmitry Lepikhin, James Qin, Dehao Chen,\\nYuanzhong Xu, Zhifeng Chen, Adam Roberts,\\nMaarten Bosma, Vincent Zhao, Yanqi Zhou,\\nChung-Ching Chang, Igor Krivokon, Will Rusch,\\nMarc Pickett, Pranesh Srinivasan, Laichee Man,\\nKathleen Meier-Hellstern, Meredith Ringel Mor-\\nris, Tulsee Doshi, Renelito Delos Santos, Toju\\nDuke, Johnny Soraker, Ben Zevenbergen, Vin-\\nodkumar Prabhakaran, Mark Diaz, Ben Hutchin-\\nson, Kristen Olson, Alejandra Molina, Erin\\nHoffman-John, Josh Lee, Lora Aroyo, Ravi\\nRajakumar, Alena Butryna, Matthew Lamm,\\nViktoriya Kuzmina, Joe Fenton, Aaron Cohen,\\nRachel Bernstein, Ray Kurzweil, Blaise Aguera-\\nArcas, Claire Cui, Marian Croak, Ed Chi, and\\nQuoc Le. 2022. LaMDA: Language models for\\ndialog applications. ArXiv:2201.08239.\\nHugo Touvron, Thibaut Lavril, Gautier Izac-\\nard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Rozière, Naman\\nGoyal, Eric Hambro, Faisal Azhar, Aurelien\\nRodriguez, Armand Joulin, Edouard Grave,\\nand Guillaume Lample. 2023a.\\nLLaMA:\\nOpen and efficient foundation language models.\\nArXiv:2302.13971.\\nHugo Touvron, Louis Martin, Kevin Stone, Pe-\\nter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal\\nBhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen,\\nGuillem Cucurull, David Esiobu, Jude Fernan-\\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-\\nthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou,\\nHakan Inan, Marcin Kardas, Viktor Kerkez,\\nMadian Khabsa, Isabel Kloumann, Artem Ko-\\nrenev, Punit Singh Koura, Marie-Anne Lachaux,\\nThibaut Lavril, Jenya Lee, Diana Liskovich,\\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor\\nMihaylov, Pushkar Mishra, Igor Molybog, Yixin\\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi\\nRungta, Kalyan Saladi, Alan Schelten, Ruan\\nSilva, Eric Michael Smith, Ranjan Subramanian,\\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor,\\nAdina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, An-\\ngela Fan, Melanie Kambadur, Sharan Narang,\\nAurelien Rodriguez, Robert Stojnic, Sergey\\nEdunov, and Thomas Scialom. 2023b. Llama\\n2: Open foundation and fine-tuned chat models.\\nArXiv:2307.09288.\\nAshish Vaswani, Noam Shazeer, Niki Parmar,\\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\\ntention is all you need. In Proc. of NeurIPS.\\nSinong Wang, Belinda Z. Li, Madian Khabsa,\\nHan\\nFang,\\nand\\nHao\\nMa.\\n2020.\\nLin-\\nformer: Self-attention with linear complexity.\\nArXiv:2006.04768.\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\\nDubey, Joshua Ainslie, Chris Alberti, Santiago\\nOntanon, Philip Pham, Anirudh Ravula, Qifan\\nWang, Li Yang, and Amr Ahmed. 2020. Big\\nBird: Transformers for longer sequences. In\\nProc. of NeurIPS.\\nA\\nAmbiguity in Multi-Document QA\\nDistractor Documents\\nFollowing past work on NaturalQuestions-Open\\n(Izacard et al., 2021; Izacard and Grave, 2021, inter\\nalia), we use a Wikipedia dump from late 2018\\nas our retrieval corpus. However, this standard\\nWikipedia dump has a small amount of temporal\\nmismatch with the NaturalQuestions annotations.\\nFor example, consider the question “what nfl\\nteam does robert griffin iii play for”. The Natu-\\nralQuestions annotated answer is “currently a free\\nagent”. However, the Wikipedia retrieval corpus\\ncontains the information that he plays for the “Balti-\\nmore Ravens”, since he was released from the team\\nbetween the Wikipedia dump’s timestamp and the\\nNaturalQuestions annotation process.\\nWe use the ambiguity annotations of Min et al.\\n(2020) to create a subset unambiguous questions.\\nExperiments on this unambiguous subset of the\\ndata show similar results and conclusions as the\\nexperiments on the full questions collection (Fig-\\nure 12).\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n60\\n65\\n70\\n75\\nAccuracy\\n20 T\\notal Retrieved Documents \\n(~4K tokens, unambiguous questions)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 12: Language model performance on a unam-\\nbiguous subset of questions.\\nB\\nRandom Distractors in\\nMulti-Document QA\\nWe also run multi-document question answering\\nexperiments with random Wikipedia documents as\\ndistractors, which allows us to ablate the impact\\nof retrieved distractors (hard negatives). Note that\\nin this setting, the the document containing the an-\\nswer can often be identified with simple heuristics\\n(e.g., lexical overlap with the query). Figure 13\\npresents the results of this experiment. Although\\nall models have higher absolute accuracy in this\\nsetting, they surprisingly still struggle to reason\\nover their entire input context, indicating that their\\nperformance degradation is not solely due to an\\ninability to identify relevant documents.\\nC\\nRandomizing Distractor Order in\\nMulti-Document QA\\nOur prompt instructs the language model to use\\nthe provided search results to answer the question.\\nThere may be a prior in the pre-training or instruc-\\ntion fine-tuning data to treat search results as sorted\\nby decreasing relevance (i.e., the documents near\\nthe beginning of the input context are more likely to\\nbe useful than those at the end). To validate that our\\nconclusions are not simply a byproduct of this bias,\\nwe run experiments with the modified instruction\\n“Write a high-quality answer for the given ques-\\ntion using only the provided search results (some\\nof which might be irrelevant). The search results\\nare ordered randomly.” In addition, we randomly\\nshuffle the k −1 distractor documents.\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n65\\n70\\n75\\n80\\nAccuracy\\n20 T\\notal Retrieved Documents\\n(~4K tokens, random distractors)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 13: Language model performance on multi-\\ndocument QA when using random distractors, rather\\nthan retrieved distractors.\\nFigure 14 presents the results of this experiment.\\nWe continue to see a U-shaped performance curve,\\nwith performance degrading when language mod-\\nels must use information in the middle of their\\ninput contexts. Comparing the results in §2.3 with\\nthose when randomizing the distractor order and\\nmentioning such in the prompt, we see that ran-\\ndomization slightly decreases performance when\\nthe relevant information is at the very beginning\\nof the context, and slightly increases performance\\nwhen using information in the middle and end of\\nthe context.\\nD\\nGPT-4 Performance\\nWe evaluate GPT-4 (8K) on a subset of 500 ran-\\ndom multi-document QA examples with 20 total\\ndocuments in each input context (Figure 15). GPT-\\n4 achieves higher absolute performance than any\\nother language model, but still shows a U-shaped\\nperformance curve—its performance is highest\\nwhen relevant information occurs at the very start\\nor end of the context, and performance degrades\\nwhen it must use information in the middle of its\\ninput context.\\nE\\nLlama-2 Performance\\nWe evaluate Llama-2 (Touvron et al., 2023b) on\\nmulti-document QA with 20 total documents in\\neach input context.\\nThe Llama tokenizer pro-\\nduces longer sequences than the tokenizers for our\\npreviously-studied models, so we discard 20 exam-\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n55\\n60\\n65\\n70\\n75\\nAccuracy\\n20 T\\notal Retrieved Documents\\n(~4K tokens, randomly ordered)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 14: Language model performance when random-\\nizing the order of the distractors (rather than presenting\\nthem in order of decreasing relevance) and mentioning\\nas such in the prompt.\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n50\\n60\\n70\\n80\\n90\\nAccuracy\\n20 T\\notal Retrieved Documents \\n(~4K tokens, 500 question sample)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\ngpt-4-0613\\nFigure 15: Although GPT-4 has higher absolute perfor-\\nmance than other models, its performance still degrades\\nwhen relevant information occurs in the middle of the\\ninput context.\\nples (out of 2655) that exceed Llama-2’s maximum\\ncontext length of 4096 tokens. We experiment with\\nmodels of varying sizes (7B, 13B, and 70B pa-\\nrameters), with and without additional supervised\\nfine-tuning and reinforcement learning from hu-\\nman feedback (“-chat-” models). The results are\\npresented in Figure 16.\\nComparing Llama-2 models of varying sizes, we\\nfind that only the larger models (13B and 70B)\\nexhibit the U-shaped performance curve (i.e., both\\nprimacy and recency bias)—the smallest Llama-\\n2 models (7B) are solely recency-biased. Given\\nthese results, we hypothesize that prior work (e.g.,\\nKhandelwal et al., 2018; Sun et al., 2021) did not\\npreviously observe any primacy bias in language\\nmodels because the models they studied were too\\nsmall (less than 1B parameters).\\nComparing between Llama-2 models with and\\nwithout additional supervised fine-tuning and re-\\ninforcement learning from human feedback, we\\nsee that additional fine-tuning dramatically im-\\nproves performance on the multi-document QA\\ntask. The 7B models with and without additional\\nfine-tuning show minimal primacy bias, and are\\nlargely recency-biased. The 13B base model has\\na dramatic primacy and recency bias—there is a\\n20-point accuracy disparity between the best- and\\nworst-case performance. Applying additional fine-\\ntuning to the 13B seems to slightly reduce this\\nbias (10-point worst-case degradation), but the bias\\nremains significant. However, the 70B models\\nwith and without additional fine-tuning have largely\\nsimilar trends (showing both primacy and recency\\nbias), and additional fine-tuning minimally changes\\nthe positional bias severity.\\n1st\\n5th\\n10th\\n15th\\n20th\\nPosition of Document with the Answer\\n20\\n30\\n40\\n50\\n60\\n70\\nAccuracy\\n20 T\\notal Retrieved Documents (~4K tokens)\\nLlama-2-7b-chat-hf\\nLlama-2-13b-chat-hf\\nLlama-2-70b-chat-hf\\nLlama-2-7b-hf\\nLlama-2-13b-hf\\nLlama-2-70b-hf\\nFigure 16: Multi-document QA performance (20 total\\ndocuments) of Llama-2 models of varying sizes (7B,\\n13B, 70B parameters), with and without additional su-\\npervised fine-tuning and reinforcement learning from\\nhuman feedback (“-chat-” models).\\nF\\nToken Counts\\nTable 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input\\ncontexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,\\nGPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)\\nuse the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,\\nmodulo some additional special tokens that do not appear in our data. As a result, the token counts for\\nthese two model families is the same in our experimental settings.\\nClosed-Book\\nOracle\\navg ± stdev\\nmax\\navg ± stdev\\nmax\\nLongChat-13B (16K)\\n55.6 ± 2.7\\n70\\n219.7 ± 48.5\\n588\\nMPT-30B\\n43.5 ± 2.2\\n58\\n187.9 ± 41.8\\n482\\nGPT-3.5-Turbo\\n15.3 ± 2.2\\n29\\n156.0 ± 41.8\\n449\\nClaude-1.3\\n15.3 ± 2.2\\n29\\n156.0 ± 41.8\\n449\\nTable 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document\\nquestion answering settings.\\n10 docs\\n20 docs\\n30 docs\\navg ± stdev\\nmax\\navg ± stdev\\nmax\\navg ± stdev\\nmax\\nLongChat-13B (16K)\\n1749.9 ± 112.4\\n2511\\n3464.6 ± 202.3\\n4955\\n5181.9 ± 294.7\\n7729\\nMPT-30B\\n1499.7 ± 88.5\\n1907\\n2962.4 ± 158.4\\n3730\\n4426.9 ± 230.5\\n5475\\nGPT-3.5-Turbo\\n1475.6 ± 86.5\\n1960\\n2946.2 ± 155.1\\n3920\\n4419.2 ± 226.5\\n6101\\nClaude-1.3\\n1475.6 ± 86.5\\n1960\\n2946.2 ± 155.1\\n3920\\n4419.2 ± 226.5\\n6101\\nTable 3: Token count statistics for each of the evaluated models on each of the document question answering\\nsettings.\\n75 KV pairs\\n140 KV pairs\\n300 KV pairs\\navg ± stdev\\nmax\\navg ± stdev\\nmax\\navg ± stdev\\nmax\\nLongChat-13B (16K)\\n5444.5 ± 19.1\\n5500\\n10072.4 ± 24.1\\n10139\\n21467.3 ± 35.9\\n21582\\nMPT-30B\\n4110.5 ± 23.8\\n4187\\n7600.9 ± 31.1\\n7687\\n16192.4 ± 46.6\\n16319\\nGPT-3.5-Turbo\\n3768.7 ± 25.6\\n3844\\n6992.8 ± 34.1\\n7088\\n14929.4 ± 50.7\\n15048\\nClaude-1.3\\n3768.7 ± 25.6\\n3844\\n6992.8 ± 34.1\\n7088\\n14929.4 ± 50.7\\n15048\\nTable 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.\\nG\\nFull Multi-Document Question Answering Results\\nThis section tabulates model performance when evaluated on the multi-document QA task with varying\\nnumbers of documents (Figure 5). “Index n” indicates performance when the document with the answer\\noccurs at position n + 1, where lower indices are closer to the start of the input context. For example,\\nindex 0 refers to performance when the document with the answer is placed at the very start of the context\\n(i.e., first amongst all documents).\\nG.1\\n10 Total Retrieved Documents\\nModel\\nIndex 0\\nIndex 4\\nIndex 9\\nClaude-1.3\\n62.9%\\n58.3%\\n59.7%\\nClaude-1.3 (100K)\\n63.1%\\n58.3%\\n59.7%\\nGPT-3.5-Turbo\\n76.8%\\n61.2%\\n62.4%\\nGPT-3.5-Turbo (16K)\\n76.9%\\n61.0%\\n62.5%\\nMPT-30B-Instruct\\n60.2%\\n56.2%\\n59.7%\\nLongChat-13B (16K)\\n72.1%\\n58.9%\\n58.5%\\nTable 5: Model performance when evaluated on the multi-document QA task with 10 total retrieved documents.\\nG.2\\n20 Total Retrieved Documents\\nModel\\nIndex 0\\nIndex 4\\nIndex 9\\nIndex 14\\nIndex 19\\nClaude-1.3\\n59.9%\\n55.9%\\n56.8%\\n57.2%\\n60.1%\\nClaude-1.3 (100K)\\n59.8%\\n55.9%\\n57.0%\\n57.4%\\n60.0%\\nGPT-3.5-Turbo\\n75.8%\\n57.2%\\n53.8%\\n55.4%\\n63.2%\\nGPT-3.5-Turbo (16K)\\n75.7%\\n57.3%\\n54.1%\\n55.4%\\n63.1%\\nMPT-30B-Instruct\\n53.7%\\n51.8%\\n52.2%\\n52.7%\\n56.3%\\nLongChat-13B (16K)\\n68.6%\\n57.4%\\n55.3%\\n52.5%\\n55.0%\\nTable 6: Model performance when evaluated on the multi-document QA task with 20 total retrieved documents.\\nG.3\\n30 Total Retrieved Documents\\nModel\\nIndex 0\\nIndex 4\\nIndex 9\\nIndex 14\\nIndex 19\\nIndex 24\\nIndex 29\\nClaude-1.3\\n59.1%\\n55.1%\\n54.8%\\n55.7%\\n56.4%\\n56.2%\\n59.9%\\nClaude-1.3 (100K)\\n59.1%\\n55.1%\\n54.9%\\n55.7%\\n56.6%\\n56.1%\\n60.0%\\nGPT-3.5-Turbo (16K)\\n73.4%\\n55.1%\\n50.5%\\n50.9%\\n51.8%\\n54.9%\\n63.7%\\nMPT-30B-Instruct\\n51.6%\\n51.3%\\n51.2%\\n49.0%\\n49.6%\\n51.3%\\n54.1%\\nLongChat-13B (16K)\\n66.9%\\n54.8%\\n52.5%\\n52.9%\\n52.2%\\n51.3%\\n55.1%\\nTable 7: Model performance when evaluated on the multi-document QA task with 30 total retrieved documents.\\n' url='https://arxiv.org/abs/2307.03172'\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://arxiv.org/abs/2401.18059\",\n",
    "    \"https://arxiv.org/abs/2307.03172\",\n",
    "]\n",
    "\n",
    "papers = fetch_multiple_papers(urls)\n",
    "for idx, paper in enumerate(papers):\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Document\n",
    "\n",
    "> _...Construction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\n",
    "texts of length 100, similar to traditional retrieval augmentation techniques._\n",
    "\n",
    "由於LLM Token limit的限制，以及太長的context會影響模型的效果。因此實務上，不論LLM是否能夠接納多長的文本，大多數的情況下都會將文本切割成較短的片段。\n",
    "\n",
    "由於RAPTOR希望捕捉文本的深層結構，因此在Chunk的顆粒度上盡可能小一些。\n",
    "作者在他的實驗中選擇了長度為100 TOKEN的Chunk，這個數字是可以根據文本的特性去調整的。\n",
    "同時，作者提到如果有句子會超過100 token，那麼該句子直接放到下一個Chunk中。這樣的設計是為了保證chunk的在語意上是完整的。\n",
    "\n",
    "我在實作中打算簡單一點，直接使用 [lammaindex SentenceSplitter](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/) 來實現這個功能。\n",
    "\n",
    "同時為了方便之後的處理，這邊我們先定義 `Node` 的結構，之後組成樹的時候會用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(BaseModel):\n",
    "    text: str\n",
    "    level: int = Field(default=0)\n",
    "    children: set[\"Node\"] = Field(default_factory=set)\n",
    "    embedding: list[float] | None = Field(default=None)\n",
    "\n",
    "    @field_validator(\"embedding\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def convert_to_np_array(cls, v):\n",
    "        if isinstance(v, list):\n",
    "            return np.array(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents(papers: list[ArXivPaper]) -> list[Node]:\n",
    "    \"\"\"Prepares a list of documents from ArXivPaper.\"\"\"\n",
    "    splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    sentences = itertools.chain.from_iterable(\n",
    "        splitter.split_text(paper.text) for paper in papers\n",
    "    )\n",
    "    nodes = [Node(text=sentence) for sentence in sentences]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(text='Published as a conference paper at ICLR 2024\\nRAPTOR: RECURSIVE ABSTRACTIVE PROCESSING\\nFOR TREE-ORGANIZED RETRIEVAL\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\\nStanford University\\npsarthi@cs.stanford.', level=0, children=set(), embedding=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = prepare_documents(papers)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed Document\n",
    "\n",
    "> _...These texts\n",
    "are then embedded using SBERT, a BERT-based encoder (multi-qa-mpnet-base-cos-v1)\n",
    "(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\n",
    "leaf nodes of our tree structure._\n",
    "\n",
    "作者在這裡使用了SBERT來對文本進行Embedding。\n",
    "這邊也採用一樣的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(docs: list[Node], embeddings) -> list[Node]:\n",
    "    \"\"\"Embeds text for each document.\"\"\"\n",
    "    for doc in tqdm(docs):\n",
    "        doc.embedding = embeddings.encode(doc.text)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8beec1ccbf4a548ad84b05d09b2b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = embed_documents(docs, embeddings=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding: (768,)\n",
      "Example of the embedding: [ 0.04584955  0.07172221 -0.01249831  0.03709074 -0.03058134]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the embedding: {docs[0].embedding.shape}\")\n",
    "print(f\"Example of the embedding: {docs[0].embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Documents\n",
    "\n",
    ">\n",
    "\n",
    "分群是RAPTOR的核心，其目的就是將前面文本切割成的小片段進行聚合，這樣可以更好的捕捉文本的深層結構。接著透過遞迴的方式，將整個文本的結構由最底層的片段逐步的聚合到最上層，最後產生一個樹狀的結構，這個結構就是RAPTOR的核心概念。\n",
    "\n",
    "作者在這裡使用了GMM來進行分群。 \n",
    "\n",
    "在實作中，大致上分成 **local clustering** 和 **global clustering** 兩個部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of global clusters: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aee1fc2a45445a491c124d9ee9c8c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 9\n"
     ]
    }
   ],
   "source": [
    "class ClusterResult(t.NamedTuple):\n",
    "    clusters_array: np.ndarray\n",
    "    n_clusters: int\n",
    "\n",
    "\n",
    "class RAPTORClustering:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reduction_components: int,\n",
    "        threshold: float,\n",
    "        random_state: int,\n",
    "        max_cluster_size: int,\n",
    "    ):\n",
    "        self.reduction_components = reduction_components\n",
    "        self.threshold = threshold\n",
    "        self.random_state = random_state\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "\n",
    "    def __call__(self, embeddings: np.ndarray):\n",
    "        embeddings = embeddings.copy()\n",
    "\n",
    "        doc_size, _ = embeddings.shape\n",
    "\n",
    "        total_cluster_count = 0\n",
    "        total_clusters_labels = [\n",
    "            np.array([], dtype=int) for _ in range(doc_size)\n",
    "        ]  # [(doc_size, 0)]\n",
    "\n",
    "        # * Global clustering\n",
    "        # Use large n_neighbors for global clustering to capture the global structure.\n",
    "        # In the author's implementation, they use n_neighbors = sqrt(embeddings_size - 1)\n",
    "        global_n_neighbors = max(int((doc_size - 1) ** 0.5), 2)\n",
    "        global_cluster_labels, global_n_clusters = self.cluster(\n",
    "            embeddings=embeddings,\n",
    "            n_components=self.reduction_components,\n",
    "            n_neighbors=global_n_neighbors,\n",
    "        )\n",
    "\n",
    "        print(f\"Number of global clusters: {global_n_clusters}\")\n",
    "\n",
    "        # * Local clustering\n",
    "        for global_cluster_idx in tqdm(\n",
    "            range(global_n_clusters), desc=\"Local Clustering...\"\n",
    "        ):\n",
    "            # Extract embeddings for each global cluster\n",
    "            embeddings_in_global_cluster = embeddings[\n",
    "                np.array([global_cluster_idx in gc for gc in global_cluster_labels])\n",
    "            ]\n",
    "\n",
    "            # Handle the empty cluster\n",
    "            # Sometimes, GMM may not find any cluster for the given threshold\n",
    "            if len(embeddings_in_global_cluster) == 0:\n",
    "                continue\n",
    "\n",
    "            # Determine if local clustering is needed\n",
    "            # If the number of embeddings in this global cluster is too small (<= reduction components + 1),\n",
    "            # further clustering is unnecessary. Assign all embeddings to a single local cluster.\n",
    "            if len(embeddings_in_global_cluster) <= self.reduction_components + 1:\n",
    "                local_cluster_labels = [\n",
    "                    np.array([0]) for _ in embeddings_in_global_cluster\n",
    "                ]\n",
    "                n_local_clusters = 1\n",
    "            else:\n",
    "                # * Perform local clustering\n",
    "                # Use small n_neighbors for local clustering to capture the local structure.\n",
    "                # In the author's implementation, they use n_neighbors = 10\n",
    "                local_cluster_labels, n_local_clusters = self.cluster(\n",
    "                    embeddings=embeddings_in_global_cluster,\n",
    "                    n_components=self.reduction_components,\n",
    "                    n_neighbors=10,\n",
    "                )\n",
    "\n",
    "            # * Update the total_clusters_label with local cluster assignments\n",
    "            for local_cluster_idx in range(n_local_clusters):\n",
    "                self.update_cluster_labels(\n",
    "                    embeddings=embeddings,\n",
    "                    embeddings_in_global_cluster=embeddings_in_global_cluster,\n",
    "                    local_cluster_label=local_cluster_labels,\n",
    "                    total_clusters_label=total_clusters_labels,\n",
    "                    local_cluster_idx=local_cluster_idx,\n",
    "                    total_cluster_count=total_cluster_count,\n",
    "                )\n",
    "\n",
    "            total_cluster_count += n_local_clusters\n",
    "\n",
    "        print(f\"Total number of clusters: {total_cluster_count}\")\n",
    "\n",
    "        return total_clusters_labels\n",
    "\n",
    "    # @tenacity.retry(stop=(tenacity.stop_after_delay(20) | tenacity.stop_after_attempt(5)))\n",
    "    def cluster(\n",
    "        self, embeddings: np.ndarray, n_components: int, n_neighbors: int = 10\n",
    "    ) -> ClusterResult:\n",
    "        # Reduce the dimensionality by UMAP\n",
    "        reduced_embeddings = umap.UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n_components,\n",
    "            metric=\"cosine\",\n",
    "        ).fit_transform(embeddings)\n",
    "\n",
    "        # Find the optimal number of clusters\n",
    "        n_clusters = self.find_cluster_count(reduced_embeddings)\n",
    "\n",
    "        # Perform clustering\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        gmm.fit(reduced_embeddings)\n",
    "\n",
    "        # Assign the cluster to each embedding\n",
    "        probs = gmm.predict_proba(reduced_embeddings)\n",
    "\n",
    "        # Given threshold, assign the cluster\n",
    "        clusters_array = np.array(\n",
    "            [np.where(prob >= self.threshold)[0] for prob in probs]\n",
    "        )  # (doc_size, 1)\n",
    "\n",
    "        return ClusterResult(clusters_array=clusters_array, n_clusters=n_clusters)\n",
    "\n",
    "    def find_cluster_count(self, embeddings: np.ndarray) -> int:\n",
    "        doc_size, _ = embeddings.shape\n",
    "        # ? Determine the maximum number of clusters, ensuring it doesn't exceed the document count.\n",
    "        # ? The number of clusters should not exceed the number of documents because:\n",
    "        # ? 1. Each document could potentially be its own cluster, but you can't have more clusters than documents.\n",
    "        # ? 2. Clustering aims to group similar documents, so having more clusters than documents is not meaningful.\n",
    "        clusters_size = min(self.max_cluster_size, doc_size)\n",
    "        n_clusters = np.arange(1, clusters_size)\n",
    "        bics = []\n",
    "        for n in n_clusters:\n",
    "            gm = GaussianMixture(n_components=n, random_state=self.random_state)\n",
    "            gm.fit(embeddings)\n",
    "            bics.append(gm.bic(embeddings))\n",
    "        return n_clusters[np.argmin(bics)]\n",
    "\n",
    "    def update_cluster_labels(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        embeddings_in_global_cluster: np.ndarray,\n",
    "        local_cluster_label: np.ndarray,\n",
    "        total_clusters_label: list[np.ndarray],\n",
    "        local_cluster_idx: int,\n",
    "        total_cluster_count: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Update the total_clusters_label with the new cluster index.\"\"\"\n",
    "\n",
    "        # Identify which embeddings belong to the current local cluster\n",
    "        is_in_local_cluster = np.array(\n",
    "            [local_cluster_idx in label for label in local_cluster_label]\n",
    "        )\n",
    "        embeddings_in_local_cluster = embeddings_in_global_cluster[is_in_local_cluster]\n",
    "\n",
    "        # Find the indices of these embeddings in the original embeddings array\n",
    "        expanded_embeddings = embeddings_in_local_cluster[:, np.newaxis]\n",
    "        comparison_matrix = embeddings == expanded_embeddings\n",
    "        matches = comparison_matrix.all(axis=-1)\n",
    "        indices = np.where(np.any(matches, axis=0))[0]\n",
    "\n",
    "        # Update the total_clusters_label with the new cluster index\n",
    "        new_cluster_index = local_cluster_idx + total_cluster_count\n",
    "        for idx in indices:\n",
    "            current_label = total_clusters_label[idx]\n",
    "            total_clusters_label[idx] = np.append(current_label, new_cluster_index)\n",
    "\n",
    "clustering = RAPTORClustering(\n",
    "    reduction_components=40,\n",
    "    threshold=THRESHOLD,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_cluster_size=MAX_CLUSTER_SIZE,\n",
    ")\n",
    "embeddings = np.array([doc.embedding for doc in docs])\n",
    "cluster_label = clustering(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of global clusters: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a723587ce2402d9f07d3625450b697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 8\n",
      "Number of global clusters: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e21a09fb7ad47a99d73505606c7bf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 11\n",
      "Number of global clusters: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f7479a9b8f4165b081d18ece0be8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 7\n",
      "Number of global clusters: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7a222f9c8f432f8331fd93b3c66852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 16\n",
      "Number of global clusters: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416bae0caad84ddda797367d9e87b002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Clustering...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 8\n"
     ]
    }
   ],
   "source": [
    "def run_clustering(\n",
    "    nodes: list[Node],\n",
    "    reduction_components: int = REDUCTION_COMPONENTS,\n",
    "    threshold: float = THRESHOLD,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    max_cluster_size: int = MAX_CLUSTER_SIZE,\n",
    "    max_token: int = MAX_TOKEN,\n",
    ") -> list[Node]:\n",
    "    \"\"\"Runs the RAPTOR clustering algorithm on the given nodes.\"\"\"\n",
    "    model = RAPTORClustering(\n",
    "        reduction_components=reduction_components,\n",
    "        threshold=threshold,\n",
    "        random_state=random_state,\n",
    "        max_cluster_size=max_cluster_size,\n",
    "    )\n",
    "\n",
    "    # * Perform clustering\n",
    "    embeddings = np.array([node.embedding for node in nodes])\n",
    "    cluster_labels = model(embeddings)\n",
    "    n_cluster_labels = np.unique(np.concatenate(cluster_labels))\n",
    "\n",
    "    result = []\n",
    "    for label in n_cluster_labels:\n",
    "        # Get the indices of the nodes that belong to this cluster\n",
    "        indices = [i for i, cluster in enumerate(cluster_labels) if label in cluster]\n",
    "\n",
    "        # Add the corresponding nodes to the node_clusters list\n",
    "        cluster_nodes = [nodes[i] for i in indices]\n",
    "\n",
    "        # Base case: if the cluster only has one node, do not attempt to recluster it\n",
    "        if len(cluster_nodes) == 1:\n",
    "            result.append(cluster_nodes)\n",
    "            continue\n",
    "\n",
    "        # Calculate the total length of the text in the nodes\n",
    "        total_length = sum([len(tokenizer.encode(node.text)) for node in cluster_nodes])\n",
    "\n",
    "        # If the total length exceeds the maximum allowed length, re-cluster this cluster\n",
    "        if total_length > max_token:\n",
    "            result.extend(run_clustering(nodes=cluster_nodes))\n",
    "        else:\n",
    "            result.append(cluster_nodes)\n",
    "\n",
    "    return result\n",
    "\n",
    "result = run_clustering(nodes=docs, reduction_components=40, max_cluster_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = sum([len(cluster) for cluster in result])\n",
    "assert size == len(docs), f\"Expected {len(docs)} nodes, but got {size} nodes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnode\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'node' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = RAPTORClustering()\n",
    "embeddings = np.array([doc.embedding for doc in docs])\n",
    "clusters_array, n_clusters = clustering.cluster(embeddings)\n",
    "\n",
    "print(f\"Number of documents: {len(clusters_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.get_num_clusters(clusters_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([cluster[0] for cluster in clusters_array]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimension Reduction (UMAP)\n",
    "\n",
    "> _...The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\n",
    "tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\n",
    "garwal et al., 2001)._\n",
    "\n",
    "Embedding之後，就可以準備分群。\n",
    "\n",
    "不過在分群之前，作者提到了SBERT的Embedding是高維的 (768)，這對於傳統的GMM模型來說是一個挑戰。\n",
    "\n",
    "因此這邊此用UMAP來對Embedding進行降維，作者降到10維。\n",
    "\n",
    "> _Our algorithm varies `n_neighbors` to create a hierar-\n",
    "chical clustering structure: it first identifies **global clusters** and then performs **local clustering** within\n",
    "these global clusters. This two-step clustering process captures a broad spectrum of relationships\n",
    "among the text data, from broad themes to specific details._\n",
    "\n",
    "作者在這裡提到了他們的算法是通過調整`n_neighbors`來創建一個層次化的聚類結構，有兩個步驟：\n",
    "1. Global Clustering： 這個步驟是為了找到全局的Cluster。\n",
    "2. Local Clustering： 在全局的Cluster中進行局部的Clustering。這樣的步驟可以捕捉到從廣泛的主題到具體細節的關係。\n",
    "\n",
    "> _Should a local cluster’s combined context ever exceed the summarization model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold._\n",
    "\n",
    "為什麼要分成global和local呢？作者在這裡提到了這樣的設計是為了捕捉到文本的深層結構。\n",
    "還有一個重要的原因是，由於token limitation，當global分群的文本太長時，沒辦法直接將其所有文本拿去summary。\n",
    "因此，作者在這裡提到了如果local cluster的文本超過了summarization model的token threshold，那麼他們會在這個cluster中再次應用clustering，直到文本的長度在summarization model的token threshold之內。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# First, we do global clustering\n",
    "n_neighbors = int(\n",
    "    (len(nodes[0].embedding) - 1) ** 0.5\n",
    ")  # large n_neighbor to capture the global structure\n",
    "umap_model = umap.UMAP(\n",
    "    n_components=10,\n",
    "    n_neighbors=n_neighbors,\n",
    "    metric=\"cosine\",\n",
    ")\n",
    "\n",
    "embeddings = np.array([node.embedding for node in nodes])\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "print(reduced_embeddings.shape)  # (sample_size, 10)\n",
    "print(reduced_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering (GMM)\n",
    "\n",
    "> _Clustering plays a key role in building the RAPTOR tree, organizing text\n",
    "segments into cohesive groups. This step groups related content together, which helps the subsequent retrieval process....One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\n",
    "belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essential because individual text segments often contain information relevant to various topics, thereby\n",
    "warranting their inclusion in multiple summaries. Our clustering algorithm is based on Gaussian Mixture Models (GMMs)..._\n",
    "\n",
    "準備後降為後的Embedding進行分群了，這是RAPTOR最核心的一個步驟。\n",
    "\n",
    "作者在這裡使用了GMM來進行分群，並且使用了soft clustering的方式，這樣可以讓一個文本屬於多個Cluster。這樣的方法，可以讓RAPTOR更好的捕捉文本的深層結構，並將相關的文本放到一起。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def find_optimal_clusters(\n",
    "    embeddings: np.ndarray, *, max_clusters: int, random_state: int\n",
    ") -> int:\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in tqdm(n_clusters, desc=\"Finding optimal number of clusters\"):\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(reduced_embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "n_global_clusters = find_optimal_clusters(\n",
    "    reduced_embeddings, max_clusters=50, random_state=random_state\n",
    ")\n",
    "print(f\"Optimal number of clusters: {n_global_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到 optimal number of clusters 之後就可以開始分群了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=n_global_clusters, warm_start=True, verbose=1)\n",
    "gmm.fit(reduced_embeddings)\n",
    "probs = gmm.predict_proba(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "total_clusters = 0\n",
    "\n",
    "for global_idx in range(n_global_clusters):\n",
    "    global_clustering_idx = np.array([global_idx in gc for gc in labels])\n",
    "    global_cluster_embeddings_ = embeddings[global_clustering_idx]\n",
    "    print(f\"Nodes in Global Cluster {global_idx}: {len(global_cluster_embeddings_)}\")\n",
    "    ####################\n",
    "    # Local clustering #\n",
    "    ####################\n",
    "\n",
    "    # sometimes, GMM may generate cluster with empty data points.\n",
    "    # In this case, we skip this local clustering.\n",
    "    if len(global_cluster_embeddings_) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get the number of local clusters\n",
    "\n",
    "    # If the cluster size is ≤ dim + 1, meaningful clustering isn't possible due to insufficient points.\n",
    "    if len(global_cluster_embeddings_) <= n_components + 1:\n",
    "        local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "        n_local_clusters = 1\n",
    "    # find the optimal number of local clusters\n",
    "    else:\n",
    "        local_cluster_embeddings_ = umap.UMAP(\n",
    "            n_components=n_components,\n",
    "            n_neighbors=10,  # use small n_neighbors to capture the local structure\n",
    "            metric=\"cosine\",\n",
    "        ).fit_transform(global_cluster_embeddings_)\n",
    "        n_local_clusters = find_optimal_clusters(\n",
    "            local_cluster_embeddings_, max_clusters=50, random_state=random_state\n",
    "        )\n",
    "        gmm = GaussianMixture(n_components=n_local_clusters, random_state=random_state)\n",
    "        gmm.fit(local_cluster_embeddings_)\n",
    "        probs = gmm.predict_proba(local_cluster_embeddings_)\n",
    "        local_clusters = [np.where(prob > threshold)[0] for prob in probs]\n",
    "\n",
    "    for local_idx in range(n_local_clusters):\n",
    "        local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "            np.array([local_idx in lc for lc in local_clusters])\n",
    "        ]\n",
    "        indices = np.where((embeddings == local_cluster_embeddings_[:, None]).all(-1))[\n",
    "            1\n",
    "        ]\n",
    "        for idx in indices:\n",
    "            all_local_clusters[idx] = np.append(\n",
    "                all_local_clusters[idx], local_idx + total_clusters\n",
    "            )\n",
    "\n",
    "    total_clusters += n_local_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_local_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "labels = np.unique(np.concatenate(all_local_clusters))\n",
    "node_clusters = []\n",
    "\n",
    "for label in labels:\n",
    "    indices = [i for i, cluster in enumerate(all_local_clusters) if label in cluster]\n",
    "    print(f\"Cluster {label}: {indices}\")\n",
    "\n",
    "    cluster_nodes = [nodes[i] for i in indices]\n",
    "\n",
    "    # Base case: if the cluster only has one node, do not attempt to recluster it\n",
    "    if len(cluster_nodes) == 1:\n",
    "        node_clusters.append(cluster_nodes)\n",
    "        continue\n",
    "\n",
    "    # Calculate the total length of the text in the nodes\n",
    "    total_length = sum([len(tokenizer.encode(node.text)) for node in cluster_nodes])\n",
    "    print(f\"Total length: {total_length}\")\n",
    "\n",
    "    # If the total length exceeds the maximum allowed length, recluster this cluster\n",
    "    if total_length > max_length_in_cluster:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAPTORClustering:\n",
    "\n",
    "    def __init__(self, n_components: int = 10, max_clusters: int = 50):\n",
    "        self.n_components = n_components\n",
    "        self.max_clusters = max_clusters\n",
    "\n",
    "    def find_optimal_clusters(self, embeddings: np.ndarray) -> int:\n",
    "        max_clusters = min(max_clusters, len(embeddings))\n",
    "        n_clusters = np.arange(1, max_clusters)\n",
    "        bics = []\n",
    "        for n in tqdm(n_clusters, desc=\"Finding optimal number of clusters\"):\n",
    "            gm = GaussianMixture(n_components=n)\n",
    "            gm.fit(embeddings)\n",
    "            bics.append(gm.bic(reduced_embeddings))\n",
    "        return n_clusters[np.argmin(bics)]\n",
    "\n",
    "    def umap(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        *,\n",
    "        n_components: int,\n",
    "        n_neighbors: int | None = None,\n",
    "        metric: str = \"cosine\",\n",
    "    ) -> np.ndarray:\n",
    "        return umap.UMAP(\n",
    "            n_neighbors=n_neighbors, n_components=n_components, metric=metric\n",
    "        ).fit_transform(embeddings)\n",
    "\n",
    "    def _cluster_embeddings(self): ...\n",
    "\n",
    "    def cluster(self, embeddings: np.ndarray, *, threshold: float = 0.1):\n",
    "\n",
    "        # Global clustering\n",
    "        n_neighbors = int(\n",
    "            (len(embeddings[0]) - 1) ** 0.5\n",
    "        )  # use large n_neighbor to capture the global structure\n",
    "        reduced_embeddings = self.umap(\n",
    "            embeddings, n_components=self.n_components, n_neighbors=n_neighbors\n",
    "        )\n",
    "\n",
    "        # Find optimal number of clusters for global clustering\n",
    "        n_global_clusters = self.find_optimal_clusters(reduced_embeddings)\n",
    "\n",
    "        # Cluster embeddings based no optimal number of clusters\n",
    "        gmm = GaussianMixture(n_components=n_global_clusters)\n",
    "        gmm.fit(embeddings)\n",
    "        probs = gmm.predict_proba(embeddings)\n",
    "\n",
    "        # Get global clusters based on threshold\n",
    "        global_clusters = [np.where(prob > threshold)[0] for prob in probs]\n",
    "\n",
    "        print(f\"Number of Global Clusters: {len(n_global_clusters)}\")\n",
    "\n",
    "        for idx, cluster in enumerate(n_global_clusters):\n",
    "            global_cluster_embeddings_ = embeddings[\n",
    "                np.array([idx in gc for gc in global_clusters])\n",
    "            ]\n",
    "            print(f\"Nodes in Global Cluster {idx}: {len(global_cluster_embeddings_)}\")\n",
    "\n",
    "            ####################\n",
    "            # Local clustering #\n",
    "            ####################\n",
    "\n",
    "            # sometimes, GMM may generate cluster with empty data points.\n",
    "            # In this case, we skip this local clustering.\n",
    "            if len(global_cluster_embeddings_) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get the number of local clusters\n",
    "\n",
    "            # If the cluster size is ≤ dim + 1, meaningful clustering isn't possible due to insufficient points.\n",
    "            if len(global_cluster_embeddings_) <= n_components + 1:\n",
    "                local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "                n_local_clusters = 1\n",
    "            # find the optimal number of local clusters\n",
    "            else:\n",
    "                local_cluster_embeddings_ = umap.UMAP(\n",
    "                    n_components=n_components,\n",
    "                    n_neighbors=10,  # use small n_neighbors to capture the local structure\n",
    "                    metric=\"cosine\",\n",
    "                ).fit_transform(global_cluster_embeddings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_AS_JUDGE_TEMPLATE = \"\"\"\\\n",
    "As a cybersecurity expert, evaluate the alert description and risk level based on the attack scenario.\n",
    "Follow these steps:\n",
    "1. Develop a scoring guideline specific to the attack scenario, with a 0 to 10 scale. Define how points are awarded based on clear criteria.\n",
    "2. Assign a score from 0 to 10, and explain how well the alert meets the criteria.\n",
    "\n",
    "ATTACK SCENARIO: ```{attack_scenario}```\n",
    "\n",
    "ALERT DESCRIPTION: ```{alert_description}```\n",
    "ALERT RISK LEVEL: ```{alert_risk_level}```\n",
    "\"\"\"\n",
    "\n",
    "llm_judge_response = llm.request_chat(\n",
    "    content=LLM_AS_JUDGE_TEMPLATE.format(\n",
    "        attack_scenario=attack_scenario,\n",
    "        alert_description=alert[\"description\"],\n",
    "        alert_risk_level=alert[\"riskLevel\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(llm_judge_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
